"""Generate a paper draft from experiment artifacts, with optional LLM drafting.

Default path composes Markdown+LaTeX from artifacts. When enabled via
config/env, an LLM drafts a full Markdown paper using section scaffolds
and a brief reflection pass for clarity/consistency.
"""

import json
import re
import os
import pathlib
import shutil
import subprocess
from typing import Any, Dict, List, Optional

from dotenv import load_dotenv
from lab.config import get, get_bool
from utils.llm_utils import chat_text_cached, LLMError


load_dotenv()

DATA_DIR = pathlib.Path("data")
RUNS_DIR = pathlib.Path("runs")
PAPER_DIR = pathlib.Path("paper")


def _ensure_dirs() -> None:
    PAPER_DIR.mkdir(parents=True, exist_ok=True)


def _read_json(path: pathlib.Path) -> Dict[str, Any]:
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}


def _best_acc(runs: List[Dict[str, Any]], name_contains: str, prefer_metric: str = "test_accuracy") -> float:
    best = 0.0
    for r in runs:
        if name_contains in str(r.get("name")):
            metrics = r.get("result", {}).get("metrics", {})
            acc = metrics.get(prefer_metric)
            if acc is None:
                acc = metrics.get("val_accuracy", 0.0)
            try:
                acc = float(acc or 0.0)
            except Exception:
                acc = 0.0
            if acc > best:
                best = acc
    return best


def _render_md(title: str, novelty: Dict[str, Any], plan: Dict[str, Any], summary: Dict[str, Any]) -> str:
    # Guard against non-dict inputs; callers may pass loose JSON content
    if not isinstance(novelty, dict):
        novelty = {}
    if not isinstance(plan, dict):
        plan = {}
    if not isinstance(summary, dict):
        summary = {}
    runs_val = summary.get("runs") if isinstance(summary, dict) else None
    runs = runs_val if isinstance(runs_val, list) else []
    baseline_acc = _best_acc(runs, "baseline")
    novelty_acc = _best_acc(runs, "novelty")
    ablation_acc = _best_acc(runs, "ablation")
    delta = novelty_acc - baseline_acc
    best = None
    try:
        best = _read_json(RUNS_DIR / "best.json")
    except Exception:
        best = None

    lines: List[str] = []
    referent = str(
        get("pipeline.write_paper.style.referent", get("pipeline.write_paper_style.referent", "This study"))
        or "This study"
    )
    lines.append(f"# {title}")
    lines.append("")
    lines.append("## Abstract")
    # No fallback abstract content: the abstract is generated only by the LLM path.
    # Leave a clear placeholder so human readers know it's intentionally omitted here.
    lines.append("Abstract TBD — generated by the LLM according to the configured abstract rules.")
    lines.append("")

    # Structured Introduction (concise prose; no 'What/Why/How' subsections)
    lines.append("## Introduction")
    goal_txt = str(
        get("project.goal", "")
        or "This study investigates a minimal, reproducible approach to the target problem under constrained training budgets."
    )
    nf = str(plan.get("novelty_focus", "") or "a lightweight conditioning mechanism within a compact baseline")
    lines.append(
        f"{goal_txt} The topic is important due to practical deployment constraints and the need for reliable performance with limited supervision and compute. "
        f"The approach operates by introducing {nf}, integrated into a lean training and evaluation workflow."
    )
    lines.append("")
    # Problems Faced
    lines.append("### Problems Faced")
    problems = novelty.get("problems") or []
    if problems:
        lines.append("Key difficulties include: " + " ".join(str(p).rstrip('.') + '.' for p in problems[:3]))
    else:
        lines.append("Typical issues include scarce labeled data, sensitivity to detector quality, and tight step budgets that limit adaptation.")
    lines.append("")
    # Current Solutions
    lines.append("### Current Solutions")
    ths_local = novelty.get("themes") or []
    if ths_local:
        desc = "; ".join(f"{t.get('name', 'Theme')}" for t in ths_local[:4])
        lines.append(f"Prevailing directions include {desc}, each balancing quality and resource demands.")
    else:
        lines.append("Prior work generally scales model capacity or uses extra supervision, which can be costly under tight budgets.")
    lines.append("")
    # Gap and Rationale
    lines.append("### Gap and Rationale")
    lines.append("Despite progress, a gap remains for methods that target the constrained regime while isolating contributions beyond capacity increases; this motivates the proposed lightweight intervention.")
    lines.append("")
    # Objectives (explicit sentences)
    lines.append("### Objectives")
    obj = str(plan.get("objective", "")).strip()
    if obj:
        lines.append(f"The first objective is {obj}.")
    else:
        lines.append("The first objective is to establish a minimal, reproducible baseline for skin‑cancer classification.")
    lines.append("The second objective is to evaluate a lightweight novelty against a minimal baseline under a small training budget.")
    lines.append("")
    lines.append("### Research Question")
    rq = "Can a small, well-scoped novelty outperform a minimal baseline on skin-cancer classification under limited compute?"
    lines.append(f"The research question is: {rq}.")
    lines.append("")
    lines.append("### Contributions")
    lines.append("The first contribution is a lean, end‑to‑end research pipeline (planning → execution → reporting) with guardrails.")
    lines.append("The second contribution is a lightweight novelty with an evaluation protocol suitable for tight budgets.")
    # Optional page reminder after Introduction
    page_reminder = bool(get("pipeline.write_paper.style.page_reminder_after_intro", get("pipeline.write_paper_style.page_reminder_after_intro", False)))
    if page_reminder:
        lines.append("")
        lines.append(_page_reminder_text())
        lines.append("")

    # Related Work
    lines.append("## Related Work")
    ths = novelty.get("themes") or []
    if ths:
        lines.append("Recent themes are summarized to contextualize this study:")
        for t in ths[:5]:
            name = t.get("name") or "Theme"
            summary = t.get("summary") or ""
            lines.append(f"{name}: {summary}")
    else:
        lines.append("Prior work spans strong baselines (e.g., ResNet variants) and modest augmentations; our focus is a minimal, reproducible path with a compact novelty.")
    lines.append("")
    # Close Related Work with an explicit Research Gap subsection
    gap_stmt = "A clear gap remains for methods that explicitly address tight compute and data constraints without relying on capacity increases."
    nfocus = str(plan.get("novelty_focus", "the proposed lightweight mechanism")) or "the proposed lightweight mechanism"
    lines.append("### Research Gap")
    lines.append(
        f"{gap_stmt} The proposed method introduces {nfocus} to directly address this gap under constrained budgets, and it is evaluated against capacity-matched controls to isolate its contribution."
    )
    lines.append("")

    # Methods with data and objective details
    lines.append("## Methodology")
    lines.append("### Architecture")
    lines.append("A compact CNN/ResNet-style backbone with a linear classification head is employed; input resolution and width follow the baseline specification.")
    lines.append("")
    lines.append("### Training Objective")
    lines.append("The objective functions used are outlined below.")
    # Try to infer task/loss from best run spec
    task_kind = None
    loss_name = None
    if best and isinstance(best, dict):
        try:
            spec_b = best.get("spec", {}) or {}
            task_kind = str(spec_b.get("task") or "").strip().lower() or None
            loss_name = str(spec_b.get("loss") or "").strip().lower() or None
        except Exception:
            task_kind = None
            loss_name = None
    # Display equations (Markdown + LaTeX)
    if task_kind == "regression":
        lines.append("### Regression Loss")
        if loss_name in {"l1", "mae"}:
            lines.append("We use the mean absolute error (MAE):")
            lines.append("$$\\mathcal{L}_{\\text{MAE}} = \\frac{1}{N} \\sum_{i=1}^{N} \\lVert y_i - \\hat{y}_i \\rVert_1.$$")
        elif loss_name in {"huber", "smooth_l1"}:
            lines.append("We use the Huber (Smooth L1) loss with threshold \\delta:")
            lines.append("$$\\mathcal{L}_{\\delta}(y, \\hat{y}) = \\begin{cases} \\tfrac{1}{2}(y-\\hat{y})^2 & \\text{if } |y-\\hat{y}| \\le \\delta \\cr \\delta |y-\\hat{y}| - \\tfrac{1}{2} \\delta^2 & \\text{otherwise.} \\end{cases}$$")
        else:
            lines.append("We use mean squared error (MSE):")
            lines.append("$$\\mathcal{L}_{\\text{MSE}} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2.$$")
    else:
        lines.append("### Classification Loss")
        if loss_name in {"bce", "bcewithlogits"}:
            lines.append("For multi-label settings, we may use binary cross-entropy with logits:")
            lines.append("$$\\mathcal{L}_{\\text{BCE}} = - \\frac{1}{N} \\sum_{i=1}^{N} \\Big[y_i \\log(\\sigma(z_i)) + (1-y_i) \\log(1-\\sigma(z_i))\\Big].$$")
        else:
            lines.append("We use the standard (multi-class) cross-entropy:")
            lines.append("$$\\mathcal{L}_{\\text{CE}} = - \\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{e^{z_{i,y_i}}}{\\sum_{c} e^{z_{i,c}}}.$$")
    lines.append("")

    lines.append("## Experimental Setup")
    lines.append("Datasets, metrics, and training budgets are defined as follows.")
    # Metrics and criteria from plan
    sc = plan.get("success_criteria") or []
    if sc:
        lines.append("Success criteria from the plan include the following deltas relative to the baseline:")
        for s in sc[:5]:
            lines.append(f"{s.get('metric','metric')}: Δ vs baseline = {s.get('delta_vs_baseline','TBD')}")
        lines.append("")
    lines.append("Experiments are configured to be CPU‑friendly with at most one epoch and at most 100 steps per run.")
    lines.append("The evaluation compares the baseline, the novelty, and the ablation, along with minor variants where applicable.")
    lines.append("")
    lines.append("### Results")
    lines.append("| Setting | Acc (test) |\n|---|---:|")
    lines.append(f"| Baseline | {baseline_acc:.4f} |")
    lines.append(f"| Novelty | {novelty_acc:.4f} |")
    lines.append(f"| Ablation | {ablation_acc:.4f} |")
    lines.append("")
    lines.append(f"Delta vs baseline: {delta:+.4f}")
    lines.append("")
    if best:
        lines.append("Best run across the experiment matrix:")
        lines.append(
            f"Name: {best.get('name')}; Validation accuracy: {best.get('result',{}).get('metrics',{}).get('val_accuracy',0.0):.4f}."
        )
        spec = best.get("spec", {})
        keep = ["model", "input_size", "batch_size", "epochs", "lr", "max_train_steps", "seed"]
        parts = [f"{k}={spec.get(k)}" for k in keep if k in spec]
        if parts:
            lines.append("Specification: " + ", ".join(parts) + ".")
        lines.append("")

    lines.append("## Ablations")
    if isinstance(plan.get("tasks"), list) and any("ablation" in str(t.get("name","")).lower() for t in plan.get("tasks", [])):
        lines.append("Ablation tasks specified in the plan are summarized and their effects reported relative to the novelty and baseline.")
    else:
        lines.append("Intended ablations are outlined to isolate the novelty effect (for example, removing a specific component or modifying an augmentation).")
    lines.append("")

    lines.append("## Discussion")
    lines.append("The novelty shows modest differences under a small compute budget. Future work can investigate broader datasets and more rigorous sweeps.")
    lines.append("")

    lines.append("## Limitations")
    lines.append("The runs are short and primarily CPU‑bound; results are indicative rather than definitive.")
    lines.append("")

    # Pipeline decision and environment info
    if isinstance(summary.get("goal_reached") if isinstance(summary, dict) else None, bool):
        lines.append(f"Decision: goal_reached = {summary.get('goal_reached')}")
        lines.append("")
    try:
        env = _read_json(RUNS_DIR / "env.json")
        if env:
            lines.append("Environment:")
            py = env.get("python", "")
            exe = env.get("executable", "")
            plat = env.get("platform", "")
            lines.append(f"Python version: {py}.")
            lines.append(f"Executable: {exe}.")
            lines.append(f"Platform: {plat}.")
            lines.append("")
    except Exception:
        pass

    # Embed accuracy chart if present
    img_path = RUNS_DIR / "accuracy.png"
    if img_path.exists():
        lines.append("## Accuracy Chart")
        lines.append("![Validation Accuracy](../runs/accuracy.png)")
        lines.append("")

    lines.append("## Conclusion")
    lines.append("A lean research pipeline with planning, execution, and reporting is presented. The structure supports quick iteration and straightforward extensions.")
    lines.append("")

    # Future Work only (no Broader Impact & Ethics)
    # Future Work (explicit section)
    lines.append("## Future Work")
    lines.append(
        "Planned next steps include scaling experiments with longer training budgets and larger datasets when resources permit; "
        "evaluating additional lightweight novelties (for example, alternative heads or mild augmentations) under the same budget; "
        "and extending tasks (for example, lesion segmentation or detection) while maintaining reproducibility and guardrails."
    )
    lines.append("")

    return "\n".join(lines)


def _cap(s: str, n: int) -> str:
    if not s:
        return ""
    return s if len(s) <= n else s[: n - 3] + "..."


def _sanitize_markdown(md: str) -> str:
    """Best-effort cleanup to avoid repeated papers/sections.

    - Keep only the first top-level title ('# ')
    - Drop any repeated second top-level title and everything after
    - For second-level sections ('## '), keep the first occurrence and drop duplicates
    """
    if not md or not isinstance(md, str):
        return md
    lines = md.splitlines()
    # Truncate after a second top-level title
    title_idxs = [i for i, l in enumerate(lines) if l.strip().startswith('# ') ]
    if len(title_idxs) >= 2:
        lines = lines[: title_idxs[1]]
    # If a Future Work section exists, truncate anything after it
    try:
        fut_idx = next(i for i, l in enumerate(lines) if l.strip().lower().startswith('## future work'))
        # keep until the next section of level 2 or top-level; else keep all
        j = fut_idx + 1
        while j < len(lines) and not (lines[j].strip().startswith('## ') or lines[j].strip().startswith('# ')):
            j += 1
        lines = lines[:j]
    except StopIteration:
        pass
    # Deduplicate '## ' sections (keep first occurrence)
    out: list[str] = []
    seen_sections: set[str] = set()
    i = 0
    n = len(lines)
    while i < n:
        l = lines[i]
        if l.strip().startswith('## '):
            sec_name = l.strip()[3:].strip().lower()
            if sec_name in seen_sections:
                # skip until next heading of level 2 or top-level title
                i += 1
                while i < n and not (lines[i].strip().startswith('## ') or lines[i].strip().startswith('# ')):
                    i += 1
                continue
            seen_sections.add(sec_name)
        out.append(l)
        i += 1
    return "\n".join(out)


def _count_words(md: str) -> int:
    if not md:
        return 0
    # Rough word counter: sequences of alphanumerics/apostrophes considered words
    return len(re.findall(r"[A-Za-z0-9']+", md))


def _maybe_expand_to_min_words(
    md: str,
    *,
    min_words: int,
    referent: str,
    req_timeout: int,
    max_tries: int,
) -> str:
    """If md has fewer than min_words, ask the LLM to expand existing sections.

    Rules:
    - Do not add new sections; preserve existing headings and their order.
    - Keep 'Future Work' as the last section; do not add 'References'.
    - No invented numbers/citations; expand via descriptions, clarifications, mechanisms, and prose depth.
    - Maintain third-person style with the given referent phrase.
    """
    try:
        cur = _count_words(md)
        if cur >= int(min_words):
            return md
        target = int(min_words)
        system = (
            "You are an expert copy editor expanding a research paper draft in Markdown.\n"
            "Preserve all existing section headings and their order.\n"
            "Do NOT add new sections; keep 'Future Work' as the last section and do not add 'References'.\n"
            "Do NOT introduce new numeric claims, datasets, or citations beyond what is present; expand by elaborating explanations and mechanisms in prose.\n"
            f"Maintain strict third-person voice (no 'we'/'I'); prefer the referent phrase '{referent}'. Use a formal academic tone; avoid hype/bombastic adjectives.\n"
            "Return FULL Markdown only (no code fences)."
        )
        user = {
            "min_words": target,
            "instructions": [
                "Expand each existing section with additional non-redundant detail and clarifications.",
                "Do not add new sections; do not rename or reorder sections.",
                "Do not add References; end at the existing 'Future Work' section.",
                "Do not invent numbers or citations; keep quantitative claims as-is.",
                "Use cohesive paragraphs; no bullet lists.",
                "Ensure the Introduction contains explicit sentences: The first objective is..., The second objective is..., The research question is..., The first contribution is..., The second contribution is...",
            ],
            "draft": _cap(md, 60000),
        }
        for _ in range(2):  # a couple of attempts to reach target length
            expanded = chat_text_cached([
                {"role": "system", "content": system},
                {"role": "user", "content": json.dumps(user, ensure_ascii=False)},
            ], temperature=0.2, timeout=req_timeout, max_tries=max_tries).strip()
            if expanded:
                expanded = _sanitize_markdown(expanded)
                if _count_words(expanded) >= target:
                    return expanded
                # Use the expanded content as the next draft for further enrichment
                user["draft"] = _cap(expanded, 120000)
        return expanded or md
    except Exception:
        return md


def _compose_abstract(*, goal: str, referent: str, baseline_acc: float, novelty_acc: float,
                      agg: Dict[str, Dict[str, float]] | None, plan: Dict[str, Any]) -> str:
    """Compose a one-paragraph abstract (150–250 words) with the specified structure.

    Sections in order (merged into a single paragraph, active voice):
    Problem (present), Gap (present), Contribution (present), Setup (past), Results (past with numbers), Implication+Scope (present).
    Includes 2–3 hard numbers when available. Avoids citations, undefined acronyms, and forward references.
    """
    # Problem (present)
    problem = (
        f"Few-shot {goal} remains challenging under tight computational budgets where data and steps are limited."
        if goal else
        "Few-shot learning under tight computational budgets remains challenging."
    )
    # Gap (present)
    gap = (
        "Prior approaches often rely on larger models or extra supervision, conflating capacity increases with genuine signal use in constrained regimes."
    )
    # Contribution (present)
    novelty_focus = str(plan.get("novelty_focus", "a lightweight, testable intervention"))
    contribution = f"This paper introduces {novelty_focus} and states a falsifiable claim: it improves accuracy beyond a capacity-matched control in the same budget."

    # Setup (past)
    setup = (
        "Experiments ran with at most one epoch and at most 100 optimization steps per run, using a fixed evaluation protocol and capacity-matched controls."
    )

    # Results (past) — compute deltas and optional 95% CIs
    numbers: list[str] = []
    try:
        delta = novelty_acc - baseline_acc
        numbers.append(f"baseline={baseline_acc:.3f}")
        numbers.append(f"novelty={novelty_acc:.3f}")
        numbers.append(f"Δ={delta:+.3f}")
        ci_txt = ""
        if agg and ("baseline" in agg) and ("novelty" in agg):
            import math
            b = agg["baseline"]; n = agg["novelty"]
            if b.get("n") and n.get("n") and b["n"] >= 2 and n["n"] >= 2:
                se = math.sqrt((b["std"]**2 / b["n"]) + (n["std"]**2 / n["n"]))
                ci = 1.96 * se
                ci_txt = f" ± {ci:.3f} (95% CI on Δ)"
        results = (
            f"Across evaluated runs, accuracy improved by {delta:+.3f}{ci_txt} absolute compared to baseline "
            f"({', '.join(numbers)})."
        )
    except Exception:
        results = "Across evaluated runs, accuracy changed relative to the baseline under the fixed protocol."

    # Implication + Scope (present)
    implication = (
        "The finding supports targeted conditioning as an effective, budget-aware mechanism; conclusions are scoped to the tested datasets and step limits."
    )

    # Assemble paragraph and trim to ~150–250 words without hard-cutting mid-sentence
    pieces = [problem, gap, contribution, setup, results, implication]
    para = " ".join(pieces)
    # Soft clamp: if overly long, cap ~260 words
    words = para.split()
    if len(words) > 260:
        para = " ".join(words[:260])
    return para


def _page_reminder_text() -> str:
    """Return a one-paragraph page reminder describing what each section explains.

    Kept concise, third-person, and prose-only (no bullets).
    """
    return (
        "Page reminder: Introduction sets context and motivation, outlines the problems faced, summarizes current solutions, states the gap and rationale, and clarifies objectives, research question, and contributions. "
        "Related Work positions the study in prior literature and ends with a 'Research Gap' subsection that states the gap and how the method fills it. Methodology details the proposed model’s architecture, components, mechanisms, and training objective. "
        "Experimental Setup specifies datasets, splits, metrics, budgets, seeds, and summarizes the model configuration. Results report baseline, novelty, and ablation outcomes with tables and analysis. "
        "Ablations isolate the effect of the proposed change. Discussion interprets findings and notes threats to validity. "
        "Limitations scope the claims. Conclusion synthesizes takeaways. Future Work outlines next steps."
    )


SECTION_TIPS = {
    "Title": [
        "Concise and informative; hint at method/setting/outcome",
        "Avoid hype; under 2 lines"
    ],
    "Abstract": [
        "Single paragraph TL;DR: problem, approach, results, takeaway",
        "No hyperbole; numbers only if provided in inputs"
    ],
    "Introduction": [
        "Context and motivation; why it matters",
        "Contributions as bullets; scope under tight compute"
    ],
    "Related Work": [
        "Compare/contrast closest work; cite when relevant",
        "Explain why baselines are appropriate"
    ],
    "Methods": [
        "Describe model/augmentations/training succinctly",
        "Keep to primitives; avoid missing details"
    ],
    "Experimental Setup": [
        "Datasets/paths/splits; metrics and evaluation",
        "Training budget: <=1 epoch; steps and seeds"
    ],
    "Results": [
        "Report numbers exactly as given; no invention",
        "Compare baseline/novelty/ablation; call out deltas"
    ],
    "Discussion": [
        "Interpret results under constraints; note trade-offs",
        "State limitations and future work"
    ],
}


def _llm_paper_md(
    novelty: Dict[str, Any],
    plan: Dict[str, Any],
    summary: Dict[str, Any],
    lit_review: Optional[str],
    *,
    min_words: int = 2000,
    section_min_paragraphs: Optional[Dict[str, int]] = None,
    req_timeout: int = 300,
    max_tries: int = 4,
) -> str:
    title = str(get("project.title", "") or "Draft: Compact Research Report")
    goal = str(get("project.goal", "your goal") or "your goal")
    novelty_small = {
        "themes": (novelty.get("themes") or [])[:10],
        "new_ideas": (novelty.get("new_ideas") or [])[:12],
        "unique_ideas": (novelty.get("unique_ideas") or [])[:12],
        "novelty_ideas": (novelty.get("novelty_ideas") or [])[:6],
        "problems": (novelty.get("problems") or [])[:6],
        "objectives": (novelty.get("objectives") or [])[:6],
        "contributions": (novelty.get("contributions") or [])[:6],
    }
    plan_small = {k: plan.get(k) for k in [
        "objective", "hypotheses", "success_criteria", "datasets", "baselines",
        "novelty_focus", "stopping_rules"
    ] if k in plan}
    runs = summary.get("runs", []) if isinstance(summary.get("runs"), list) else []
    compact_runs: List[Dict[str, Any]] = []
    for r in runs[:50]:
        compact_runs.append({
            "name": r.get("name"),
            "metrics": r.get("result", {}).get("metrics", {}),
        })
    # Compose LLM system prompt with referent style
    referent = str(
        get("pipeline.write_paper.style.referent", get("pipeline.write_paper_style.referent", "This study"))
        or "This study"
    )
    system = (
        "You are an expert research writer drafting a Markdown paper under tight compute constraints.\n"
        f"Project goal: {goal}.\n"
        "Return ONLY Markdown (no code fences); use LaTeX math ($$...$$) for equations.\n"
        "Audience and tone: peer-reviewed journal style (objective, precise, well-structured, formal tone), not a blog post.\n"
        f"Length: write at least {min_words} words overall.\n"
        "Include sections in this order with required structure: \n"
        "1) Title\n"
        "2) Abstract\n"
        "3) Introduction (cohesive prose; do NOT include 'What/Why/How' subsections). Within the Introduction include explicit sentences: 'The first objective is ...', 'The second objective is ...', 'The research question is ...', 'The first contribution is ...', 'The second contribution is ...'.\n"
        "4) Related Work (situate within themes; avoid invented claims; END with a 'Research Gap' subsection that clearly states the gap and how the method fills it)\n"
        "5) Methodology (proposed model only; architecture/components/mechanisms and the training objective with explicit equations CE/BCE/MSE/Huber; highly detailed)\n"
        "6) Experimental Setup (datasets/splits/metrics/budget/seeds, and a model configuration summary with backbone, input size, params/epochs/steps/lr)\n"
        "7) Results (tables + textual analysis; compare baseline/novelty/ablation; include quantitative results, robustness, efficiency, and error analysis)\n"
        "8) Ablations (if available; otherwise rationale for future ablations)\n"
        "9) Discussion (interpretation, threats to validity)\n"
        "10) Limitations\n"
        "11) Conclusion\n"
        "12) Future Work (this is the LAST section; do not include any sections after it)\n"
        "Grounding: use ONLY provided data; do NOT invent numbers, citations, or URLs. If a value is missing, write 'TBD'.\n"
        "Constraints: experiments are <=1 epoch with small steps; keep claims modest and reproducible.\n"
        f"Style: objective and third-person (no 'we', 'I', or 'our'); when a subject is needed, use the referent phrase '{referent}'; use cohesive paragraphs; do NOT use bullet or numbered lists anywhere; avoid bombastic adjectives and hype (e.g., 'novel', 'state-of-the-art', 'remarkable', 'extraordinary', 'extremely', 'dramatically')."
        "\n\nAbstract requirements (strict): one paragraph, 150–250 words, active voice. No citations, undefined acronyms, or forward references. Order the content: Problem (present), Gap (present), Contribution (present), Setup (past), Results (past with 2–3 hard numbers and ±95% CI when available), Implication+Scope (present). Avoid future tense in the abstract."
        "\n\nRelated Work requirements: at least 8 paragraphs; end the section with a concise paragraph that clearly states the research gap and how the proposed method fills that gap (no bullets)."
        "\nMethodology requirements: at least 8 paragraphs; include the following subsections and content in prose (no bullets): Overview; Components; Mechanism; Mathematical Formulation (explicit symbols and equations); Hyperparameters and Training Schedule; Implementation Details (layers, shapes, activations, normalization, initialization)."
        "\nExperimental Setup requirements: explicitly name the dataset(s) and paths if present; list the evaluation metrics; and summarize the model configuration (backbone, input size, params/epochs/steps/learning rate) using available spec; use 'TBD' when missing; do not invent numbers."
        "\nResults requirements: at least 8 paragraphs; restate the evaluation metrics; report 2–3 hard numbers per comparison (deltas and ±95% CI when available); include sub-analysis paragraphs for (a) Quantitative Results (per-metric, per-split if available), (b) Robustness (sensitivity to detector/crop/seed, misalignment tests), (c) Efficiency (latency/FLOPs/params/wall-clock within budget), and (d) Error Analysis (common failure modes with concrete examples in prose). Tie interpretations to the configuration and budget constraints."
        "\nDiscussion requirements: at least 8 paragraphs; synthesize findings, explain mechanisms consistent with the data, compare to the research gap and how it is filled, state threats to validity (internal/external/statistical), practical implications and limitations, and outline trade-offs versus alternatives — all scoped to datasets and budgets actually tested."
    )
    tips_lines = []
    for sec, tips in SECTION_TIPS.items():
        tips_lines.append(f"- {sec} tips: " + "; ".join(tips))
    smp = section_min_paragraphs or {}
    sec_rules = {
        "Abstract": int((smp.get("Abstract") or 1) or 1),
        "Introduction": int((smp.get("Introduction") or 3) or 3),
        "Related Work": int((smp.get("Related Work") or 2) or 2),
        "Methodology": int((smp.get("Methodology") or smp.get("Methods") or 3) or 3),
        "Experimental Setup": int((smp.get("Experimental Setup") or 2) or 2),
        "Results": int((smp.get("Results") or 3) or 3),
        "Ablations": int((smp.get("Ablations") or 1) or 1),
        "Discussion": int((smp.get("Discussion") or 2) or 2),
        "Limitations": int((smp.get("Limitations") or 1) or 1),
        "Conclusion": int((smp.get("Conclusion") or 1) or 1),
    }
    # Optional page-reminder flag for LLM output after Introduction
    page_reminder_flag = bool(get("pipeline.write_paper.style.page_reminder_after_intro", get("pipeline.write_paper_style.page_reminder_after_intro", False)))
    user = {
        "title": title,
        "tips": tips_lines,
        "novelty": novelty_small,
        "plan": plan_small,
        "runs": compact_runs,
        "lit_review": _cap(lit_review or "", 18000),
        "require": [
            "Report numbers exactly as given (val/test metrics only if present)",
            "Cite only if citation strings are provided (else omit)",
            "Keep each section substantial; meet minimal paragraph counts",
            "Use a formal academic tone; avoid hype and rhetorical adjectives (e.g., 'novel', 'state-of-the-art', 'remarkable', 'extraordinary', 'extremely', 'dramatically').",
            "Ensure the Introduction includes the exact sentences: 'The first objective is ...', 'The second objective is ...', 'The research question is ...', 'The first contribution is ...', 'The second contribution is ...'.",
            f"Target overall length >= {min_words} words",
            "Related Work must end with a 'Research Gap' subsection that clearly states the gap and how the proposed method fills it (prose only).",
            "Experimental Setup must include dataset names/paths, the evaluation metrics, and a model configuration summary (backbone, input size, params/epochs/steps/lr) using available spec or 'TBD'.",
            "Results must restate evaluation metrics and include 2–3 hard numbers; discuss in light of configuration and budgets.",
            "Discussion must connect the outcomes to the dataset/evaluation/configuration and clearly tie back to the gap and how it is filled.",
            ("After the Introduction, include a short 'Page reminder' paragraph that states what each subsequent section covers (Introduction recap; Related Work; Methodology; Experimental Setup; Results; Ablations; Discussion; Limitations; Conclusion; Future Work). One paragraph, no bullets." if page_reminder_flag else ""),
            "Do not include a 'References' section; stop after 'Future Work'.",
            "Do not introduce 'What/Why/How' subsections anywhere in the paper.",
        ],
        "section_min_paragraphs": sec_rules,
        "self_check": "Before responding, scan for invented numbers/claims and remove them; ensure headings present; return Markdown only.",
    }
    text = chat_text_cached([
        {"role": "system", "content": system},
        {"role": "user", "content": json.dumps(user, ensure_ascii=False)},
    ], temperature=0.2, timeout=req_timeout, max_tries=max_tries)
    draft = text.strip()

    reflect_system = (
        "You are a careful reviewer. Revise the draft Markdown to improve clarity, fix inconsistencies, and remove any invented content.\n"
        "Maintain the same sections and satisfy minimal paragraph counts if any are below target.\n"
        f"Enforce third-person voice only (no 'we', 'I', or 'our'); prefer the referent phrase '{referent}' for subject; remove any bullet or numbered lists and convert them to prose.\n"
        "Use a formal academic tone; avoid hype/bombastic adjectives (e.g., 'novel', 'state-of-the-art', 'remarkable', 'extraordinary', 'extremely', 'dramatically').\n"
        "Return FULL Markdown (no code fences)."
    )
    reflect_user = {
        "draft": _cap(draft, 40000),
        "checks": [
            "Numbers appear only where provided in runs/summary",
            "Methods/setup match constraints and plan",
            "No missing sections; remove hype; keep objective formal tone",
            "Ensure the Introduction contains the explicit sentences: The first objective is..., The second objective is..., The research question is..., The first contribution is..., The second contribution is..."
        ],
        "self_check": "Before responding, ensure the output is complete Markdown, not partial, and contains no fenced code.",
    }
    try:
        revised = chat_text_cached([
            {"role": "system", "content": reflect_system},
            {"role": "user", "content": json.dumps(reflect_user, ensure_ascii=False)},
        ], temperature=0.0, timeout=req_timeout, max_tries=max_tries)
        md = revised.strip()
        if md:
            return md
    except LLMError:
        pass
    return draft


def _latex_escape_text(s: str) -> str:
    # Minimal escaping suitable for prose; math blocks handled separately
    return (
        s.replace("\\", "\\textbackslash{}")
         .replace("&", "\\&")
         .replace("%", "\\%")
         .replace("#", "\\#")
         .replace("_", "\\_")
         .replace("{", "\\{")
         .replace("}", "\\}")
    )


def _md_to_latex(md: str) -> str:
    """Very small, deterministic Markdown→LaTeX for our paper structure.

    - Drops the first '# Title' line (LaTeX \\title handles it)
    - Maps '## '→\\section, '### '→\\subsection
    - Preserves $$...$$ math blocks verbatim
    - Escapes special chars in prose lines; skips Markdown tables and images (handled elsewhere)
    """
    lines = md.splitlines()
    out: List[str] = []
    i = 0
    # Drop top-level markdown title if present
    if lines and lines[0].lstrip().startswith('# '):
        i = 1
    in_math = False
    while i < len(lines):
        l = lines[i]
        ls = l.strip()
        # Math block toggling
        if ls.startswith('$$'):
            out.append(l)
            in_math = not in_math
            i += 1
            continue
        if in_math:
            out.append(l)
            i += 1
            continue
        # Headings
        if ls.startswith('### '):
            hdr = ls[4:].strip()
            out.append(f"\\subsection{{{_latex_escape_text(hdr)}}}")
        elif ls.startswith('## '):
            hdr = ls[3:].strip()
            out.append(f"\\section{{{_latex_escape_text(hdr)}}}")
        # Skip images and raw markdown tables
        elif ls.startswith('![') or ls.startswith('|'):
            pass
        # Blank line → paragraph break
        elif ls == '':
            out.append('')
        else:
            out.append(_latex_escape_text(l))
        i += 1
    return "\n".join(out)


def _render_latex(title: str, md_path: pathlib.Path, latex_table: str | None = None, include_fig: bool = False, nocite_all: bool = False) -> str:
    nocite_cmd = "\\nocite{*}" if nocite_all else ""
    try:
        md = md_path.read_text(encoding="utf-8")
    except Exception:
        md = ""
    body = _md_to_latex(md) if md else "This is an auto-generated draft. See paper.md."
    fig_block = (
        "\\begin{figure}[h!]\\centering\\includegraphics[width=\\linewidth]{../runs/accuracy.png}\\caption{Validation Accuracy}\\end{figure}"
        if include_fig else ""
    )
    res_table = ("\\section*{Results}\n" + latex_table) if latex_table else ""
    return f"""\\documentclass[11pt]{{article}}
\\usepackage[margin=1in]{{geometry}}
\\usepackage[T1]{{fontenc}}
\\usepackage{{lmodern}}
\\usepackage{{graphicx}}
\\usepackage[numbers]{{natbib}}
\\title{{{title}}}
\\author{{Auto Writer}}
\\date{{}}
\\begin{{document}}
\\maketitle
{body}
{res_table}
{fig_block}
{nocite_cmd}
\\bibliographystyle{{plainnat}}
\\bibliography{{refs}}
\\end{{document}}
"""


def _sanitize_bib_key(s: str) -> str:
    keep = "".join(c for c in s if c.isalnum())
    return keep[:40] or "ref"


def build_bibtex_from_csv(csv_path: pathlib.Path, out_dir: pathlib.Path) -> pathlib.Path | None:
    if not csv_path.exists():
        return None
    import csv
    rows: List[Dict[str, str]] = []
    try:
        with open(csv_path, "r", encoding="utf-8", newline="") as f:
            r = csv.DictReader(f)
            for row in r:
                rows.append(row)
    except Exception:
        return None
    if not rows:
        return None
    out_dir.mkdir(parents=True, exist_ok=True)
    bib_path = out_dir / "refs.bib"
    lines: List[str] = []
    for i, row in enumerate(rows, start=1):
        title = str(row.get("title") or f"Untitled-{i}")
        year = str(row.get("year") or "")
        doi = str(row.get("doi") or "")
        key = _sanitize_bib_key((title or "") + year)
        lines.append(f"@article{{{key},")
        lines.append(f"  title={{ {title} }},")
        if year:
            lines.append(f"  year={{ {year} }},")
        if doi:
            lines.append(f"  doi={{ {doi} }},")
        lines.append("}")
        lines.append("")
    bib_path.write_text("\n".join(lines), encoding="utf-8")
    return bib_path


def _aggregate_from_summary(summary: Dict[str, Any]) -> Dict[str, Dict[str, float]]:
    import math
    runs = summary.get("runs", []) if isinstance(summary.get("runs"), list) else []
    groups = {"baseline": [], "novelty": []}
    for r in runs:
        name = str(r.get("name") or "")
        base = name.split("_rep", 1)[0]
        if base in groups:
            try:
                acc = float(r.get("result", {}).get("metrics", {}).get("val_accuracy", 0.0) or 0.0)
                groups[base].append(acc)
            except Exception:
                pass
    out: Dict[str, Dict[str, float]] = {}
    for k, vals in groups.items():
        if not vals:
            continue
        n = float(len(vals))
        mean = sum(vals) / n
        var = sum((v - mean) ** 2 for v in vals) / n
        std = math.sqrt(var)
        out[k] = {"mean": mean, "std": std, "n": n}
    return out


def render_mean_std_table_md(agg: Dict[str, Dict[str, float]]) -> List[str]:
    lines = ["| Setting | N | Mean | Std |", "|---|---:|---:|---:|"]
    for k in ("baseline", "novelty"):
        if k in agg:
            a = agg[k]
            lines.append(f"| {k} | {int(a['n'])} | {a['mean']:.4f} | {a['std']:.4f} |")
    return lines


def render_mean_std_table_tex(agg: Dict[str, Dict[str, float]]) -> str:
    rows = []
    for k in ("baseline", "novelty"):
        if k in agg:
            a = agg[k]
            rows.append(f"{k} & {int(a['n'])} & {a['mean']:.4f} & {a['std']:.4f} \\ ")
    if not rows:
        return ""
    return (
        "\\begin{table}[h!]\\centering\\begin{tabular}{lrrr}\\hline Setting & N & Mean & Std \\ \\hline "
        + " ".join(rows)
        + " \\hline\\end{tabular}\\caption{Mean and standard deviation of validation accuracy.}\\end{table}"
    )


def try_pdflatex(tex_path: pathlib.Path) -> None:
    exe = shutil.which("pdflatex")
    if not exe:
        return
    try:
        subprocess.run([exe, "-interaction=nonstopmode", tex_path.name], cwd=str(tex_path.parent), check=True, timeout=120)
    except Exception:
        pass


def main() -> None:
    _ensure_dirs()
    novelty = _read_json(DATA_DIR / "novelty_report.json")
    plan = _read_json(DATA_DIR / "plan.json")
    summary = _read_json(RUNS_DIR / "summary.json")
    # Robustness: guard against files that contain valid JSON but not an object (e.g., a string)
    if not isinstance(novelty, dict):
        novelty = {}
    if not isinstance(plan, dict):
        plan = {}
    if not isinstance(summary, dict):
        summary = {}

    project_title = str(get("project.title", "") or "").strip()
    project_topic = str(get("project.topic", "Skin-Cancer Classification") or "Skin-Cancer Classification")
    title = project_title or f"A Minimal Novelty for {project_topic}"
    if novelty.get("new_ideas"):
        idea = str(novelty["new_ideas"][0])[:60]
        if idea:
            title = f"Toward: {idea}"

    # Optional LLM drafting mode
    use_llm = (
        get_bool("pipeline.write_paper.llm.enable", False)
        or get_bool("pipeline.write_paper_llm.enable", False)
        or (
        str(os.getenv("WRITE_PAPER_LLM", "")).lower() in {"1", "true", "yes"}
        )
    )
    lit_review: Optional[str] = None
    try:
        lp = DATA_DIR / "lit_review.md"
        if lp.exists():
            lit_review = lp.read_text(encoding="utf-8")
    except Exception:
        lit_review = None

    # Enforce LLM-only writing (no fallback composer)
    if not use_llm:
        raise LLMError("no fallback: LLM writer required (enable pipeline.write_paper_llm.enable or set WRITE_PAPER_LLM=1)")

    # Optional length/section guidance from config
    try:
        min_words = int(get("pipeline.write_paper_llm.min_words", 2000) or 2000)
    except Exception:
        min_words = 2000
    try:
        sec_min = get("pipeline.write_paper_llm.section_min_paragraphs", None)
        sec_min = sec_min if isinstance(sec_min, dict) else None
    except Exception:
        sec_min = None
    try:
        req_timeout = int(get("pipeline.write_paper_llm.request_timeout", 600) or 600)
    except Exception:
        req_timeout = 600
    try:
        req_retries = int(get("pipeline.write_paper_llm.request_retries", 4) or 4)
    except Exception:
        req_retries = 4
    # Generate via LLM; on error, hard-fail (no fallback)
    md = _llm_paper_md(
        novelty,
        plan,
        summary,
        lit_review,
        min_words=min_words,
        section_min_paragraphs=sec_min,
        req_timeout=req_timeout,
        max_tries=req_retries,
    )
    # Post-process to remove repeated sections or duplicate papers
    md = _sanitize_markdown(md)
    # Ensure Introduction explicitly contains objectives, research question, and contributions in the required phrasing
    def _enforce_intro(md_text: str) -> str:
        try:
            # Quick scan for required phrases
            needs = [
                "The first objective is",
                "The second objective is",
                "The research question is",
                "The first contribution is",
                "The second contribution is",
            ]
            if all(p in md_text for p in needs):
                return md_text
            # Extract Introduction section only
            lines = md_text.splitlines()
            intro_start = None
            for i, l in enumerate(lines):
                if l.strip().lower() == "## introduction":
                    intro_start = i
                    break
            if intro_start is None:
                return md_text
            j = intro_start + 1
            while j < len(lines) and not lines[j].startswith("## "):
                j += 1
            intro_md = "\n".join(lines[intro_start:j])
            system = (
                "You are a careful editor. Rewrite the Introduction section to include explicit sentences with formal tone:\n"
                "- 'The first objective is ...'\n- 'The second objective is ...'\n- 'The research question is ...'\n- 'The first contribution is ...'\n- 'The second contribution is ...'\n"
                "Keep existing content and structure otherwise; do not add bullets or new sections; maintain third-person and formal tone. Return only the updated Introduction section in Markdown."
            )
            revised = chat_text_cached([
                {"role": "system", "content": system},
                {"role": "user", "content": intro_md},
            ], temperature=0.0, timeout=req_timeout, max_tries=req_retries).strip()
            if revised and revised.lower().startswith("## introduction"):
                new = lines[:intro_start] + revised.splitlines() + lines[j:]
                return "\n".join(new)
            return md_text
        except Exception:
            return md_text
    md = _enforce_intro(md)
    # Enforce minimum length (at least config min_words, and no less than 10k as per preference)
    try:
        referent = str(
            get("pipeline.write_paper.style.referent", get("pipeline.write_paper_style.referent", "This study"))
            or "This study"
        )
    except Exception:
        referent = "This study"
    min_enforced = max(10000, int(min_words or 0))
    md = _maybe_expand_to_min_words(
        md,
        min_words=min_enforced,
        referent=referent,
        req_timeout=req_timeout,
        max_tries=req_retries,
    )
    md_path = PAPER_DIR / "paper.md"
    md_path.write_text(md, encoding="utf-8")

    agg = _aggregate_from_summary(summary)
    latex_table = render_mean_std_table_tex(agg) if agg else None
    tex_path = PAPER_DIR / "main.tex"
    img_path = RUNS_DIR / "accuracy.png"
    # Try to build refs.bib; then include all entries via \nocite{*} if present
    csv_default = pathlib.Path("abstract_screen_deepseek.csv")
    try:
        bib_path = build_bibtex_from_csv(csv_default, PAPER_DIR)
    except Exception:
        bib_path = None
    nocite_all = bool(bib_path and pathlib.Path(bib_path).exists())
    tex_path.write_text(
        _render_latex(title, md_path, latex_table=latex_table, include_fig=img_path.exists(), nocite_all=nocite_all),
        encoding="utf-8",
    )

    try_pdflatex(tex_path)
    print(f"[DONE] Wrote paper draft to {md_path} and {tex_path}")


if __name__ == "__main__":
    main()
