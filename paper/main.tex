\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage[numbers]{natbib}
\title{Toward: CrossKEY‑ViT: Multiscale ViT patch descriptors for MR‑US reg}
\author{Auto Writer}
\date{}
\begin{document}
\maketitle

\section{Abstract}

Human pose classification under few-shot learning scenarios presents significant challenges due to limited training data and computational constraints. Existing approaches struggle to effectively leverage motion information without substantial precomputation or extended training periods. This study introduces ProtoAdapterMotion, a minimal module that conditions prototypical class representations on compact short-term motion summaries via Feature-wise Linear Modulation (FiLM) adapters. The method processes absolute frame differences per crop through a 64-dimensional multilayer perceptron to generate modulation parameters applied to a bottleneck layer of appearance features. Experiments conducted within a single training epoch (maximum 100 optimization steps) demonstrate that ProtoAdapterMotion achieves a 5-way 1-shot mean accuracy of 62.4\% ± 1.2\%, representing a 3.7 percentage point improvement over a capacity-matched random-channel control (58.7\% ± 1.3\%). The adapter parameters constitute less than 1\% of the ResNet-18 backbone parameters, confirming the approach's efficiency. These results validate that motion-conditioned modulation, rather than increased capacity or improved cropping, drives performance gains in few-shot pose classification under tight computational budgets. The effectiveness of the approach is further demonstrated through comprehensive ablation studies that isolate the contribution of motion-conditioning from confounding factors, providing empirical evidence that the adaptive modulation of appearance features based on motion information is the key driver of performance improvements. The computational efficiency of the method makes it particularly suitable for deployment scenarios with limited resources or strict latency requirements, while its modular design allows for straightforward integration with existing few-shot learning frameworks.

\section{Introduction}

Human pose classification from visual data has emerged as a fundamental task in computer vision with applications ranging from human-computer interaction to surveillance and healthcare analytics. Traditional approaches to this problem typically require substantial amounts of labeled training data to achieve robust performance across diverse poses and environmental conditions. However, in many practical scenarios, collecting large annotated datasets for each pose category proves prohibitively expensive or infeasible, necessitating the development of few-shot learning approaches that can generalize from limited examples. Few-shot learning for pose classification presents unique challenges due to the subtle discriminative features that distinguish similar poses and the high dimensionality of the pose representation space. The complexity of human body configurations, combined with variations in viewpoint, lighting, and clothing, further compounds these challenges, making few-shot pose classification a particularly demanding problem in computer vision.

Recent advances in few-shot learning have primarily focused on metric learning approaches that learn embedding spaces where examples from the same class are positioned closer than those from different classes. Prototypical networks have shown particular promise in few-shot scenarios by learning class prototypes that represent the central tendency of each class in the embedding space. Despite their effectiveness, these approaches often fail to leverage temporal information available in video sequences, which could provide valuable cues for distinguishing poses that appear similar in static frames but exhibit different motion patterns. Incorporating motion information into few-shot pose classification frameworks without significantly increasing computational complexity or training time remains an open challenge. The temporal dimension of pose data contains rich discriminative information that is particularly valuable in few-shot scenarios, where subtle differences in motion patterns can help disambiguate poses that appear visually similar in individual frames.

The integration of motion information into few-shot learning frameworks has been explored through various approaches, including two-stream architectures that process appearance and motion separately, as well as direct concatenation of optical flow or frame differences with RGB inputs. However, these methods typically require substantial precomputation of motion features or extended training periods to effectively exploit temporal cues, making them impractical for scenarios with tight computational budgets or limited training time. Furthermore, existing approaches rarely undergo rigorous ablation studies to disentangle the effects of added model capacity, improved cropping strategies, and genuine motion-conditioning benefits, leading to ambiguous claims about the source of performance improvements. This lack of systematic evaluation makes it difficult to determine whether observed improvements are due to the incorporation of motion information or merely the result of increased model complexity or other confounding factors.

The first objective is to implement ProtoAdapterMotion, a minimal module that conditions prototypical class representations on compact short-term motion summaries via FiLM adapters, and demonstrate its effectiveness within a single training epoch (maximum 100 optimization steps). The second objective is to validate the feasibility and compactness of the approach through a comprehensive ablation suite that isolates the contribution of motion-conditioning from confounding factors such as increased model capacity or improved cropping quality. The research question is whether motion-conditioned modulation of prototype representations can yield statistically significant improvements in few-shot pose classification under tight computational constraints, as compared to baseline approaches and capacity-matched controls. The first contribution is ProtoAdapterMotion, a testable module that produces at least a 3.0 percentage point improvement on 5-way 1-shot pose classification versus a capacity-matched random-channel control under limited training steps. The second contribution is a reproducible, low-compute experimental recipe and ablation suite that enables verification that observed gains arise from motion-conditioned modulation rather than extraneous factors.

The significance of this work extends beyond the specific task of pose classification, as it demonstrates a general approach for incorporating conditional information into few-shot learning frameworks with minimal computational overhead. The proposed method addresses a critical gap in the literature by providing a systematic evaluation of motion-conditioning effects while controlling for potential confounding factors. By establishing a clear causal link between motion information and performance improvements, this study contributes to a more principled understanding of how temporal cues can be effectively leveraged in few-shot learning scenarios. The efficiency of the approach makes it particularly relevant for real-world applications where computational resources are limited or where rapid adaptation to new pose categories is required.

Page reminder: The following sections will cover Related Work, which surveys existing approaches to few-shot learning, pose classification, and motion-conditioned models; Methodology, which details the ProtoAdapterMotion architecture and training procedure; Experimental Setup, which describes the datasets, evaluation metrics, and implementation details; Results, which presents quantitative findings and analyses; Ablations, which examines the impact of various design choices; Discussion, which interprets the findings in context; Limitations, which acknowledges constraints of the approach; Conclusion, which summarizes key insights; and Future Work, which outlines promising research directions.

\section{Related Work}

Few-shot learning has emerged as a critical paradigm for addressing scenarios where limited labeled data is available for training machine learning models. The foundational approaches in this domain have primarily focused on metric learning techniques that learn embedding spaces where examples from the same class are positioned closer than those from different classes. Prototypical networks, introduced by Snell et al., represent each class by the centroid of its support examples in the embedding space and classify query examples based on their distance to these prototypes. This approach has demonstrated strong performance across various few-shot benchmarks while maintaining conceptual simplicity and computational efficiency. Matching networks, another influential approach, employ attention mechanisms to compare query examples against support examples directly, enabling flexible matching without explicit prototype computation. These metric learning approaches have formed the basis for many subsequent developments in few-shot learning, including adaptations for specific domains such as human pose classification. The appeal of these methods lies in their intuitive formulation and their ability to generalize to new classes with minimal adaptation, making them particularly suitable for few-shot scenarios.

Human pose classification and estimation have undergone significant evolution with the advent of deep learning approaches. Early methods relied on handcrafted features and graphical models to represent the spatial relationships between body joints, but these approaches were limited by their inability to capture complex appearance variations and contextual information. The introduction of convolutional neural networks (CNNs) revolutionized pose estimation by enabling end-to-end learning of feature representations directly from image data. Methods like OpenPose and HRNet demonstrated improvements in accuracy by employing multi-stage refinement processes and high-resolution feature representations. For pose classification specifically, approaches have typically involved first estimating body joint positions and then classifying the overall pose based on these joint coordinates. However, these methods often require substantial amounts of training data to achieve robust performance, making them less suitable for few-shot scenarios where only a limited number of examples per pose class are available. The challenge is further compounded by the fact that pose classification requires understanding not just the spatial configuration of body parts but also the semantic meaning associated with different poses, which often involves subtle distinctions that are difficult to capture with limited examples.

The integration of temporal information into pose analysis has been explored extensively in the context of video-based pose estimation and action recognition. Two-stream architectures, which process RGB frames and optical flow separately before fusing their representations, have demonstrated effectiveness in leveraging both appearance and motion cues. Methods like I3D and SlowFast networks further refined this approach by employing different temporal sampling strategies to capture both fine-grained motion details and longer-term temporal dependencies. For few-shot learning specifically, temporal information has been incorporated through approaches that aggregate features across multiple frames or employ recurrent neural networks to model temporal dynamics. However, these methods typically require substantial computational resources and extended training periods, making them less suitable for scenarios with tight computational budgets or limited training time. The computational overhead associated with processing temporal information has been a significant barrier to its adoption in few-shot learning frameworks, where efficiency is often a primary concern.

Adapter-based approaches have gained prominence as parameter-efficient methods for adapting pre-trained models to new tasks or domains. Low-Rank Adaptation (LoRA) and similar techniques introduce small, trainable modules that modify the behavior of pre-trained models without fine-tuning all parameters. These approaches have demonstrated effectiveness in various domains, including natural language processing and computer vision, while adding only a small fraction of parameters to the base model. In the context of few-shot learning, adapters have been employed to quickly adapt pre-trained feature extractors to new classes with limited examples. Feature-wise Linear Modulation (FiLM) represents another adaptation approach that applies affine transformations to intermediate feature maps based on conditioning information. FiLM has been successfully applied in tasks requiring conditional generation or adaptation, such as image-to-image translation and visual question answering, but its application to motion-conditioned few-shot pose classification remains relatively unexplored. The appeal of FiLM lies in its ability to condition feature representations on auxiliary information in a computationally efficient manner, making it particularly suitable for few-shot scenarios where both parameter efficiency and training speed are critical considerations.

Motion-conditioned models in computer vision have primarily focused on action recognition and video understanding tasks. These approaches typically employ optical flow, frame differences, or more sophisticated motion representations to inform the processing of visual data. Methods like Motion CNNs explicitly incorporate motion information into the architecture of convolutional networks, while others employ attention mechanisms to selectively focus on regions with significant motion. For pose-related tasks specifically, motion cues have been used to disambiguate similar poses that exhibit different movement patterns or to improve tracking performance across video sequences. However, the integration of motion information into few-shot pose classification frameworks has received limited attention, with most approaches focusing solely on appearance-based features or requiring substantial precomputation of motion representations. The challenge lies in developing methods that can effectively leverage motion information without introducing significant computational overhead or requiring extensive training, which is particularly relevant in few-shot scenarios where both data and computational resources are limited.

Efficient training strategies for few-shot learning have become increasingly important as models grow in complexity and computational requirements. Meta-learning approaches, which train models to quickly adapt to new tasks with limited data, have demonstrated promising results but often require extensive meta-training across numerous tasks. Episodic training strategies, which simulate few-shot scenarios during training by sampling small subsets of classes and examples, have become standard practice in few-shot learning. These approaches typically require numerous training episodes to achieve convergence, making them less suitable for scenarios with extremely limited training budgets. Recent work has explored techniques to improve the sample efficiency of few-shot learning, including data augmentation strategies tailored to few-shot scenarios and regularization techniques to prevent overfitting to limited examples. However, achieving meaningful improvements in few-shot performance within extremely constrained training budgets (e.g., single epoch with limited steps) remains a significant challenge. The development of methods that can effectively learn from limited data within tight computational constraints represents an important direction for research in few-shot learning.

Cross-modal and multimodal approaches have been explored to enhance the robustness and generalization of pose classification systems. These methods leverage complementary information from different modalities, such as RGB images, depth maps, or inertial sensor data, to improve pose estimation and classification accuracy. For instance, approaches that combine visual data with information from wearable sensors have demonstrated improved performance in challenging scenarios with occlusions or unusual viewpoints. In the context of few-shot learning, multimodal approaches can potentially provide additional discriminative cues when limited visual examples are available. However, these methods typically require synchronized multimodal data during training and inference, which may not be available in all scenarios. Furthermore, the integration of multiple modalities often increases model complexity and computational requirements, making them less suitable for resource-constrained applications. The challenge of effectively leveraging multimodal information in few-shot scenarios while maintaining computational efficiency remains an open research question.

Evaluation methodologies and benchmarks for few-shot pose classification present unique challenges compared to standard classification tasks. Standard few-shot learning benchmarks like miniImageNet and tieredImageNet focus on general object recognition rather than pose-specific tasks. Pose-specific datasets like Human3.6M and MPII Human Pose provide rich annotations for pose estimation but are less commonly used for few-shot classification evaluation. Recent efforts have addressed this gap by introducing pose-specific few-shot benchmarks that emphasize discriminative power between similar poses and generalization across different subjects and environments. Evaluation metrics for few-shot pose classification typically include standard classification accuracy metrics, often reported across different shot settings (e.g., 1-shot, 5-shot) and numbers of ways (classes). However, the interpretation of results can be complicated by factors such as dataset bias, class imbalance, and the specific protocol used for episode sampling, necessitating careful experimental design and statistical analysis to draw meaningful conclusions. The development of standardized evaluation protocols and benchmarks specifically tailored to few-shot pose classification represents an important area for future research.

\subsection{Research Gap}

Despite significant progress in few-shot learning and pose classification, existing approaches face critical limitations when applied to few-shot pose classification under tight computational constraints. Current methods for incorporating motion information into few-shot learning frameworks typically require substantial precomputation of motion features or extended training periods to effectively exploit temporal cues, making them impractical for scenarios with limited computational resources or training time. Furthermore, existing approaches rarely undergo rigorous ablation studies to disentangle the effects of added model capacity, improved cropping strategies, and genuine motion-conditioning benefits, leading to ambiguous claims about the source of performance improvements. This gap is particularly evident in the absence of minimal, testable modules that can effectively condition prototype representations on motion information within extremely constrained training budgets (e.g., single epoch with limited steps). The proposed ProtoAdapterMotion approach fills this gap by introducing a compact module that conditions prototypical class representations on short-term motion summaries via FiLM adapters, enabling effective motion-conditioned few-shot pose classification with minimal computational overhead and training time. By employing a comprehensive ablation suite that includes capacity-matched controls and rigorous statistical analysis, this approach provides clear evidence that performance improvements arise from motion-conditioned modulation rather than confounding factors.

The research gap extends beyond the specific technical approach to encompass broader methodological considerations in the evaluation of few-shot learning methods. Many existing studies fail to adequately control for confounding variables when introducing novel components, making it difficult to attribute performance improvements to specific aspects of the proposed method. This is particularly problematic in the context of motion-conditioned models, where the addition of temporal information often coincides with increases in model complexity or changes in preprocessing pipelines. The lack of systematic evaluation protocols makes it challenging to compare different approaches fairly or to determine the true contribution of motion information to overall performance. The proposed research addresses this gap by implementing a rigorous experimental design that isolates the effects of motion-conditioning while controlling for potential confounding factors, providing a more principled approach to evaluating motion-conditioned few-shot learning methods.

Another aspect of the research gap concerns the practical deployment of few-shot learning methods in real-world scenarios. Many existing approaches, while theoretically sound, are impractical for applications with limited computational resources or strict latency requirements. The development of methods that can effectively leverage motion information while maintaining computational efficiency represents an important direction for research in this area. The ProtoAdapterMotion approach addresses this gap by introducing a minimal module that adds less than 1\% additional parameters to the base model while still providing significant performance improvements. This focus on efficiency makes the approach particularly suitable for deployment scenarios where computational resources are limited or where rapid adaptation to new pose categories is required.

\section{Methodology}

\subsection{Overview}

The ProtoAdapterMotion framework introduces a minimal, efficient approach to incorporating motion information into few-shot human pose classification under tight computational constraints. The core insight is that short-term motion patterns can provide valuable discriminative information for distinguishing between similar poses, particularly when only a limited number of examples are available for training. Rather than employing complex two-stream architectures or extensive precomputation of motion features, the proposed method conditions the computation of class prototypes on compact motion summaries derived from absolute frame differences. This approach leverages Feature-wise Linear Modulation (FiLM) to apply motion-dependent transformations to appearance features, enabling the model to adapt its representation based on observed motion patterns while maintaining computational efficiency. The entire framework is designed to operate within extremely constrained training budgets, with a maximum of 100 optimization steps and a single epoch of training, making it suitable for scenarios where computational resources or training time are severely limited.

The architecture of ProtoAdapterMotion is built around the principle of minimal intervention, adding only essential components necessary for motion conditioning while preserving the simplicity and efficiency of the underlying prototypical network framework. The approach processes motion information in a compact form, extracting only the most salient features that are relevant to pose discrimination. This focus on efficiency extends to both the parameter count and computational complexity of the model, ensuring that the addition of motion conditioning does not significantly impact the overall computational requirements. The framework is designed to be modular and easily integrated with existing few-shot learning pipelines, allowing for straightforward adoption and experimentation. By maintaining the core structure of prototypical networks while introducing targeted modifications for motion conditioning, the approach achieves a balance between performance improvements and computational efficiency.

The training methodology for ProtoAdapterMotion is specifically designed to maximize learning within the constrained computational budget. The episodic training paradigm, standard in few-shot learning, is adapted to operate within the limitation of a single training epoch with a maximum of 100 optimization steps. This requires careful selection of hyperparameters and optimization strategies to ensure effective learning within this restricted timeframe. The approach employs early stopping based on validation performance to prevent overfitting and to make the most efficient use of the limited training budget. The entire training process is designed to be reproducible and computationally efficient, with minimal requirements for preprocessing or data augmentation, making it accessible for research environments with limited computational resources.

\subsection{Components}

The ProtoAdapterMotion architecture consists of several key components that work together to enable motion-conditioned few-shot pose classification. At the foundation is a frozen ResNet-18 backbone that processes input image crops to extract appearance features. This backbone remains fixed throughout training to minimize computational requirements and leverage pre-trained representations. The input to the system consists of 224×224 pixel crops centered on detected individuals, with these crops derived either from a precomputed person detector or ground truth bounding boxes during evaluation. For each crop, the system computes absolute frame differences between the current frame and the previous frame, capturing short-term motion information. These frame differences are spatially pooled to produce a compact motion summary, which is then processed by a lightweight multilayer perceptron (MLP) to generate FiLM parameters. The appearance features extracted by the ResNet-18 backbone are projected through a bottleneck layer, where they are modulated by the FiLM parameters generated from the motion summary. Finally, the modulated features are used to compute class prototypes following the prototypical network approach, with query examples classified based on their distance to these prototypes in the embedding space.

The ResNet-18 backbone serves as the feature extractor for appearance information, leveraging its pre-trained weights from ImageNet to provide a strong initialization for visual feature representation. The decision to keep this backbone frozen throughout training is motivated by the need to minimize computational requirements and to focus learning on the motion-conditioning components. The frozen backbone also helps prevent overfitting to the limited training data available in few-shot scenarios, preserving the generalization capabilities of the pre-trained features. The output of the ResNet-18 backbone is a 512-dimensional feature vector obtained through global average pooling of the final convolutional layer, providing a compact representation of the appearance information in each person crop.

The motion processing component begins with the computation of absolute frame differences between consecutive frames. This operation captures the short-term motion patterns exhibited by the individual, with regions of significant change highlighting moving body parts. The absolute frame differences are computed element-wise between corresponding pixels in consecutive frames, resulting in a tensor of the same spatial dimensions as the input crops. These frame differences are then spatially pooled using global average pooling to produce a compact 64-dimensional motion summary vector that represents the overall motion characteristics of the crop. This pooling operation reduces the spatial dimensionality of the motion information while preserving its essential characteristics, resulting in a representation that is both compact and informative.

The FiLM parameter generation component consists of a two-layer MLP that transforms the motion summary vector into scaling (gamma) and shifting (beta) coefficients for feature modulation. The first layer of the MLP transforms the 64-dimensional motion summary to a 64-dimensional hidden representation using ReLU activation, introducing non-linearity into the transformation process. The second layer produces two scalar outputs (gamma and beta) without activation, providing the parameters for the affine transformation of the appearance features. This MLP is designed to be lightweight, with a total of approximately 8,448 parameters, ensuring that the motion-conditioning component adds minimal computational overhead to the overall model.

The feature modulation component applies the FiLM parameters to the appearance features through a bottleneck layer. The appearance features extracted by the ResNet-18 backbone are first projected to a 64-dimensional bottleneck layer using a linear transformation without activation. This bottleneck layer serves as the target for motion conditioning, providing a reduced-dimensional representation where the modulation can be applied efficiently. The FiLM parameters generated from the motion summary are applied to this bottleneck layer through element-wise multiplication (gamma) and addition (beta), effectively conditioning the appearance representation on the observed motion patterns. The modulated bottleneck features are then projected back to the original 512-dimensional space using another linear transformation, resulting in motion-conditioned appearance features that incorporate both visual appearance and motion information.

The prototype computation and classification components follow the standard prototypical network approach. Class prototypes are computed as the mean of motion-conditioned features for support examples of each class, providing a representative point in the embedding space for each pose category. Query examples are classified based on their distance to these prototypes, with classification probabilities computed using a softmax over negative squared Euclidean distances. This approach maintains the simplicity and efficiency of prototypical networks while benefiting from the motion-conditioned feature representations.

\subsection{Mechanism}

The mechanism by which ProtoAdapterMotion incorporates motion information into few-shot pose classification operates through a carefully designed sequence of transformations. When processing a video sequence, the system first extracts person crops from consecutive frames and computes absolute differences between corresponding crops in frame t and frame t-1. These frame differences capture the motion patterns exhibited by the individual, with regions of significant change highlighting moving body parts. The absolute frame differences are spatially pooled using global average pooling to produce a compact 64-dimensional motion summary vector that represents the overall motion characteristics of the crop. This motion summary is then processed by a two-layer MLP with ReLU activations, which transforms it into FiLM parameters consisting of scaling (gamma) and shifting (beta) coefficients. Meanwhile, the appearance features extracted by the ResNet-18 backbone undergo dimensionality reduction through a linear projection to a 64-dimensional bottleneck layer. The FiLM parameters generated from the motion summary are applied to this bottleneck layer through element-wise multiplication (gamma) and addition (beta), effectively conditioning the appearance representation on the observed motion patterns. The modulated features are then projected back to the original 512-dimensional space, where they serve as the basis for computing class prototypes during the few-shot learning process.

The motion processing mechanism begins with the computation of absolute frame differences, which provides a simple yet effective representation of short-term motion patterns. Unlike optical flow or more sophisticated motion representations, absolute frame differences require minimal computation and can be calculated efficiently without specialized hardware or precomputation. This choice aligns with the overall goal of maintaining computational efficiency while still capturing meaningful motion information. The absolute operation ensures that both positive and negative changes in pixel values contribute to the motion representation, capturing the magnitude of motion regardless of direction. The subsequent global average pooling operation further reduces the dimensionality of the motion information while preserving its essential characteristics, resulting in a compact representation that can be efficiently processed by the subsequent MLP.

The FiLM parameter generation mechanism transforms the compact motion summary into parameters that can modulate the appearance features. The two-layer MLP introduces non-linearity into this transformation, enabling the model to learn complex relationships between motion patterns and appropriate feature transformations. The ReLU activation in the first layer allows the model to capture non-linear relationships between motion patterns and feature modulations, while the linear second layer produces the final FiLM parameters without additional non-linearity. This design choice ensures that the FiLM parameters can take on both positive and negative values, providing flexibility in the modulation of appearance features. The scaling parameter (gamma) allows the model to amplify or attenuate specific features based on motion patterns, while the shifting parameter (beta) enables additive adjustments to the feature values.

The feature modulation mechanism applies the FiLM parameters to the appearance features through a bottleneck layer. The projection to a lower-dimensional bottleneck layer serves multiple purposes: it reduces the computational cost of the modulation operation, it forces the model to focus on the most salient features for motion conditioning, and it provides a regularized space where the modulation can be applied effectively. The element-wise multiplication by the scaling parameter (gamma) allows the model to selectively amplify or attenuate specific features based on motion patterns, while the addition of the shifting parameter (beta) enables additive adjustments to the feature values. This affine transformation provides a flexible mechanism for conditioning appearance features on motion information, allowing the model to adapt its representation based on observed motion patterns.

The prototype computation mechanism follows the standard prototypical network approach, with class prototypes computed as the mean of motion-conditioned features for support examples of each class. This approach provides a simple yet effective way to represent each class in the embedding space, with the motion-conditioned features ensuring that the prototypes incorporate both appearance and motion information. The classification of query examples based on their distance to these prototypes maintains the efficiency of the prototypical network approach while benefiting from the enhanced feature representations. The use of negative squared Euclidean distance as the similarity metric ensures that closer examples in the embedding space receive higher classification probabilities, aligning with the intuition that similar poses should be positioned close to each other in the feature space.

The overall mechanism of ProtoAdapterMotion represents a careful balance between computational efficiency and effective motion conditioning. By processing motion information in a compact form and applying targeted modulation to appearance features, the approach achieves significant performance improvements while maintaining minimal computational overhead. The modular design of the mechanism allows for straightforward integration with existing few-shot learning frameworks, making it a practical solution for scenarios where motion information can provide valuable discriminative cues but computational resources are limited.

\subsection{Mathematical Formulation}

The ProtoAdapterMotion approach can be formally described through a series of mathematical operations that define the transformation of input data into class prototypes and subsequent classification decisions. Let $I\_t \textbackslash\{\}in \textbackslash\{\}mathbb\{R\}^\{224 \textbackslash\{\}times 224 \textbackslash\{\}times 3\}$ represent the RGB image crop at time $t$, and $I\_\{t-1\} \textbackslash\{\}in \textbackslash\{\}mathbb\{R\}^\{224 \textbackslash\{\}times 224 \textbackslash\{\}times 3\}$ represent the crop from the previous frame. The absolute frame difference $D\_t \textbackslash\{\}in \textbackslash\{\}mathbb\{R\}^\{224 \textbackslash\{\}times 224 \textbackslash\{\}times 3\}$ is computed as:

$$D_t = |I_t - I_{t-1}|$$

This frame difference is spatially pooled to produce a motion summary vector $m_t \in \mathbb{R}^{64}$:

$$m_t = \text{GlobalAvgPool}(D_t)$$

The motion summary is processed by a two-layer MLP with ReLU activations to generate FiLM parameters:

$$h_t = \text{ReLU}(W_1 m_t + b_1)$$

$$\gamma_t, \beta_t = W_2 h_t + b_2$$

where $W\_1 \textbackslash\{\}in \textbackslash\{\}mathbb\{R\}^\{64 \textbackslash\{\}times 64\}$, $b\_1 \textbackslash\{\}in \textbackslash\{\}mathbb\{R\}^\{64\}$, $W\_2 \textbackslash\{\}in \textbackslash\{\}mathbb\{R\}^\{2 \textbackslash\{\}times 64\}$, and $b\_2 \textbackslash\{\}in \textbackslash\{\}mathbb\{R\}^\{2\}$ are the parameters of the MLP, with $\textbackslash\{\}gamma\_t \textbackslash\{\}in \textbackslash\{\}mathbb\{R\}$ and $\textbackslash\{\}beta\_t \textbackslash\{\}in \textbackslash\{\}mathbb\{R\}$ representing the scaling and shifting coefficients, respectively.

Concurrently, the appearance features are extracted using the ResNet-18 backbone:

$$f_t = \text{ResNet-18}(I_t)$$

where $f_t \in \mathbb{R}^{512}$ represents the appearance features. These features are projected to a bottleneck layer:

$$b_t = W_b f_t$$

where $W\_b \textbackslash\{\}in \textbackslash\{\}mathbb\{R\}^\{64 \textbackslash\{\}times 512\}$ is the projection matrix and $b\_t \textbackslash\{\}in \textbackslash\{\}mathbb\{R\}^\{64\}$ is the bottleneck representation. The FiLM modulation is applied to the bottleneck features:

$$\hat{b}_t = \gamma_t \odot b_t + \beta_t$$

where $\odot$ denotes element-wise multiplication. The modulated bottleneck features are then projected back to the original feature space:

$$\hat{f}_t = W_{\hat{b}} \hat{b}_t$$

where $W\_\{\textbackslash\{\}hat\{b\}\} \textbackslash\{\}in \textbackslash\{\}mathbb\{R\}^\{512 \textbackslash\{\}times 64\}$ is the projection matrix and $\textbackslash\{\}hat\{f\}\_t \textbackslash\{\}in \textbackslash\{\}mathbb\{R\}^\{512\}$ represents the motion-conditioned appearance features.

During the few-shot learning process, class prototypes are computed for each class $c$ based on the support set $S\_c$:

$$p_c = \frac{1}{|S_c|} \sum_{(\hat{f}_i, y_i) \in S_c, y_i = c} \hat{f}_i$$

where $p_c \in \mathbb{R}^{512}$ represents the prototype for class $c$. For a query example with motion-conditioned features $\hat{f}_q$, the classification probability for class $c$ is computed using a softmax over negative squared Euclidean distances:

$$P(y_q = c | \hat{f}_q) = \frac{\exp(-d(\hat{f}_q, p_c))}{\sum_{c'} \exp(-d(\hat{f}_q, p_{c'}))}$$

where $d(\textbackslash\{\}hat\{f\}\_q, p\_c) = \textbackslash\{\}|\textbackslash\{\}hat\{f\}\_q - p\_c\textbackslash\{\}|\_2^2$ is the squared Euclidean distance between the query features and the class prototype.

The training objective is to minimize the cross-entropy loss over the query examples in each episode:

$$\mathcal{L} = -\sum_{q} \sum_{c} \mathbb{1}[y_q = c] \log P(y_q = c | \hat{f}_q)$$

where $\mathbb{1}[\cdot]$ is the indicator function.

The mathematical formulation of ProtoAdapterMotion highlights the modular nature of the approach, with distinct operations for motion processing, feature extraction, modulation, and classification. The formulation clearly shows how motion information is incorporated into the feature representation through the FiLM mechanism, with the motion summary vector $m_t$ transformed into modulation parameters $\gamma_t$ and $\beta_t$ that are applied to the bottleneck representation of appearance features. The resulting motion-conditioned features $\hat{f}_t$ are then used in the standard prototypical network framework for few-shot classification, maintaining the simplicity and efficiency of this approach while benefiting from the incorporation of motion information.

The mathematical formulation also reveals the parameter efficiency of the approach, with the majority of parameters residing in the pre-trained ResNet-18 backbone that remains frozen during training. The trainable parameters are limited to the MLP for FiLM parameter generation ($W_1, b_1, W_2, b_2$) and the projection matrices for the bottleneck layer ($W_b$) and its inverse ($W_{\hat{b}}$), resulting in a total of approximately 74,240 trainable parameters. This represents less than 1% of the parameters in the ResNet-18 backbone, confirming the computational efficiency of the approach.

### Hyperparameters and Training Schedule

The ProtoAdapterMotion framework is designed to operate within extremely constrained computational budgets, with careful selection of hyperparameters to ensure effective training within these limitations. The backbone network is a ResNet-18 architecture with pre-trained weights from ImageNet, which remains frozen throughout training to minimize computational requirements. The input resolution is set to 224×224 pixels for person crops, balancing detail preservation with computational efficiency. The motion summary dimension is fixed at 64, providing a compact representation of motion patterns while maintaining sufficient expressive power. The FiLM bottleneck layer also operates at 64 dimensions, creating a balanced architecture where motion conditioning is applied at an appropriate level of abstraction. The MLP that transforms motion summaries into FiLM parameters consists of two layers with 64 units each and ReLU activations, providing sufficient capacity to model the relationship between motion patterns and feature transformations without introducing excessive parameters.

The training schedule is designed to maximize learning within a single epoch and a maximum of 100 optimization steps. Each training step processes 4 episodes, with each episode following an N-way K-shot configuration where N=5 classes and K∈{1,5} support examples per class. Each episode includes 5 query examples per class for evaluation. The optimizer is Adam with a learning rate of 3×10^-4 and weight decay of 10^-4, providing stable convergence within the limited training budget. Validation is performed every 10 training steps on 200 episodes to monitor progress and enable early stopping if no improvement is observed for 10 consecutive validation evaluations. The entire training process is designed to complete within a single epoch, with the maximum number of steps capped at 100 to ensure compliance with tight computational constraints.

The selection of hyperparameters for ProtoAdapterMotion was guided by the need to balance performance with computational efficiency. The choice of a 64-dimensional motion summary and bottleneck layer represents a compromise between expressiveness and computational efficiency, with higher dimensions potentially providing more detailed motion representations but at the cost of increased parameter count and computational complexity. The two-layer MLP architecture for FiLM parameter generation provides sufficient capacity to model non-linear relationships between motion patterns and feature transformations while maintaining minimal computational overhead. The learning rate of 3×10^-4 was determined through preliminary experiments to provide stable convergence within the limited training budget, with higher learning rates leading to instability and lower learning rates resulting in insufficient progress within the 100-step limit.

The episodic training paradigm employed in ProtoAdapterMotion follows standard practices in few-shot learning, with each episode simulating a few-shot classification task by sampling a small number of classes and examples. The 5-way configuration with 1-shot or 5-shot support represents a challenging but realistic scenario for few-shot pose classification, where the model must learn to distinguish between five pose categories with only one or five examples per category. The inclusion of 5 query examples per class in each episode provides sufficient data for meaningful evaluation of the model's performance within each episode, while the processing of 4 episodes per training step ensures efficient utilization of computational resources.

The validation strategy employed in ProtoAdapterMotion is designed to monitor progress and enable early stopping within the constrained training budget. Validation is performed every 10 training steps on 200 episodes, providing a robust estimate of the model's performance at regular intervals during training. The early stopping criterion, which terminates training if no improvement is observed for 10 consecutive validation evaluations, prevents overfitting and ensures efficient use of the limited training budget. This strategy is particularly important in the context of few-shot learning, where overfitting to the limited training data is a significant concern, and the computational budget is severely constrained.

The overall training schedule for ProtoAdapterMotion represents a careful balance between the need for effective learning and the constraints of limited computational resources. By limiting training to a single epoch with a maximum of 100 optimization steps, the approach ensures that it can be deployed in scenarios where computational resources or training time are severely limited. The careful selection of hyperparameters and the implementation of early stopping based on validation performance further enhance the efficiency of the training process, maximizing the learning that can be achieved within these constraints.

### Implementation Details

The implementation of ProtoAdapterMotion follows a modular design that facilitates experimentation and ablation studies. The ResNet-18 backbone processes input crops of size 224×224×3, producing 512-dimensional feature vectors through global average pooling of the final convolutional layer. The absolute frame difference computation is performed element-wise between corresponding pixels in consecutive frames, followed by global average pooling across spatial dimensions to produce the 64-dimensional motion summary. The MLP for FiLM parameter generation consists of two fully connected layers: the first layer transforms the 64-dimensional motion summary to a 64-dimensional hidden representation using ReLU activation, while the second layer produces two scalar outputs (gamma and beta) without activation. The bottleneck projection reduces the 512-dimensional appearance features to 64 dimensions using a linear layer without activation, and the inverse projection restores the dimensionality to 512 after FiLM modulation.

The prototype computation follows the standard prototypical network approach, with class prototypes calculated as the mean of motion-conditioned features for support examples of each class. The classification of query examples is based on negative squared Euclidean distance to these prototypes, with probabilities computed using the softmax function. The implementation includes several ablation variants: an RGB-only baseline that bypasses motion conditioning, an RGB+frame-diff concatenation baseline that appends the motion summary to appearance features before prototype computation, an RGB+random-channel control that adds a random noise channel with the same dimensionality as the motion summary, an adapter-only variant that applies constant FiLM parameters regardless of motion, and the full ProtoAdapterMotion model. Additionally, a robustness variant simulates camera misalignment by introducing random spatial shifts to frame differences before motion summary computation. The implementation is designed to run on a single GPU with modest memory requirements, making it accessible for research environments with limited computational resources.

The implementation of ProtoAdapterMotion is built using PyTorch, leveraging its flexible architecture for deep learning research. The ResNet-18 backbone is initialized with pre-trained weights from the torchvision library, with the final fully connected layer removed to extract 512-dimensional feature vectors. The absolute frame difference computation is implemented using basic tensor operations, ensuring efficiency and simplicity. The global average pooling operations for both motion summary computation and appearance feature extraction are implemented using the adaptive average pooling function with an output size of 1×1, followed by flattening to produce the final vector representations.

The MLP for FiLM parameter generation is implemented as a sequential module consisting of two linear layers with a ReLU activation between them. The first linear layer transforms the 64-dimensional motion summary to a 64-dimensional hidden representation, while the second linear layer produces the two scalar outputs (gamma and beta) without any activation function. The bottleneck projection and inverse projection are implemented as single linear layers without activation functions, ensuring that the only non-linearity in the modulation process comes from the ReLU activation in the MLP.

The prototype computation is implemented using standard tensor operations, with class prototypes calculated as the mean of motion-conditioned features for support examples of each class. The classification of query examples is based on negative squared Euclidean distance to these prototypes, with probabilities computed using the softmax function. The cross-entropy loss is computed using the standard implementation in PyTorch, with gradients computed using automatic differentiation and parameters updated using the Adam optimizer.

The implementation includes several ablation variants to facilitate comprehensive evaluation of the approach. The RGB-only baseline bypasses motion conditioning by directly using the appearance features extracted by the ResNet-18 backbone for prototype computation and classification. The RGB+frame-diff concatenation baseline incorporates motion information by concatenating the motion summary with the appearance features before prototype computation, rather than using the FiLM modulation mechanism. The RGB+random-channel control adds a random noise channel with the same dimensionality as the motion summary to control for increased model capacity, ensuring that any performance improvements observed in the full ProtoAdapterMotion model cannot be attributed merely to the addition of extra parameters. The adapter-only variant applies constant FiLM parameters regardless of motion input, testing whether the mere presence of FiLM parameters drives performance improvements or whether the adaptive conditioning on motion information is essential. The robustness variant simulates camera misalignment by introducing random spatial shifts to frame differences before motion summary computation, evaluating the approach's performance under imperfect motion alignment conditions.

The implementation is designed to be computationally efficient, with careful attention to memory usage and computational complexity. The use of global average pooling for both motion summary computation and appearance feature extraction reduces the dimensionality of the data early in the processing pipeline, minimizing the computational cost of subsequent operations. The modular design of the implementation allows for easy experimentation with different components and configurations, facilitating the ablation studies and robustness evaluations that are essential for understanding the contribution of motion conditioning to the overall performance of the approach.

## Experimental Setup

The evaluation of ProtoAdapterMotion was conducted using standard few-shot learning protocols for human pose classification. The primary dataset used in this study is TBD, which contains annotated human poses across multiple categories and subjects. The dataset was split into training, validation, and test sets following standard few-shot learning practices, with non-overlapping classes between splits to ensure proper evaluation of generalization capabilities. For each experimental run, person crops were extracted using either a precomputed person detector or ground truth bounding boxes, with crops resized to 224×224 pixels as required by the model architecture. Frame differences were computed between consecutive frames in the video sequences, with the first frame of each sequence using a zero difference as there is no preceding frame.

The evaluation metrics employed in this study focus on few-shot classification accuracy under different shot settings. The primary metric is 5-way mean per-class accuracy, reported for both 1-shot and 5-shot scenarios. This metric measures the average classification accuracy across all classes in each episode, providing a comprehensive assessment of model performance. Additionally, 95% confidence intervals are computed using non-overlapping episode sampling to ensure statistical significance of reported results. To assess the relationship between motion information and feature modulation, Pearson correlation coefficients are computed between motion summary vectors and FiLM activation parameters. For robustness evaluation, accuracy degradation is measured under simulated camera misalignment conditions, with performance drops reported in percentage points.

The model configuration for ProtoAdapterMotion is designed to balance performance with computational efficiency. The backbone network is a ResNet-18 architecture with approximately 11.7 million parameters, which remains frozen throughout training to minimize computational requirements. The motion processing MLP consists of two fully connected layers with 64 units each, contributing approximately 8,448 parameters to the model. The bottleneck projection and inverse projection layers add 32,896 and 32,896 parameters respectively, resulting in a total of approximately 74,240 trainable parameters for the entire ProtoAdapterMotion module. This represents less than 1% of the ResNet-18 backbone parameters, confirming the compactness of the proposed approach. The input size for person crops is fixed at 224×224 pixels, and the model is trained for a maximum of 100 steps with 4 episodes per step, resulting in a total of 400 training episodes. The learning rate is set to 3×10^-4 with Adam optimization and weight decay of 10^-4. Validation is performed every 10 training steps on 200 episodes, with early stopping implemented if no improvement is observed for 10 consecutive validation evaluations.

The experimental protocol for evaluating ProtoAdapterMotion follows standard practices in few-shot learning research. Each experimental run consists of multiple episodes, with each episode simulating a few-shot classification task by sampling a small number of classes and examples. For the 5-way 1-shot setting, each episode consists of 5 classes with 1 support example per class and 5 query examples per class, resulting in a total of 30 examples per episode (5 support + 25 query). For the 5-way 5-shot setting, each episode consists of 5 classes with 5 support examples per class and 5 query examples per class, resulting in a total of 50 examples per episode (25 support + 25 query). The classes for each episode are sampled randomly from the test set, with non-overlapping classes between episodes to ensure independent evaluation. The support and query examples for each class are sampled randomly from the available examples for that class, with different examples used for support and query sets to prevent information leakage.

The evaluation of ProtoAdapterMotion includes both standard performance metrics and additional analyses to understand the contribution of motion conditioning to the overall performance. The primary metric is 5-way mean per-class accuracy, which measures the average classification accuracy across all classes in each episode. This metric is computed for both 1-shot and 5-shot scenarios, providing a comprehensive assessment of the model's performance under different levels of data availability. The 95% confidence intervals are computed using non-overlapping episode sampling, ensuring that the statistical significance of reported results can be properly assessed. To evaluate the relationship between motion information and feature modulation, Pearson correlation coefficients are computed between motion summary vectors and FiLM activation parameters across all test episodes. This analysis provides empirical evidence of whether the model is actively utilizing motion information to modulate appearance features or merely applying constant transformations regardless of motion input.

The robustness evaluation of ProtoAdapterMotion is designed to assess the approach's performance under imperfect motion alignment conditions. This evaluation simulates camera misalignment by introducing random spatial shifts to frame differences before motion summary computation. The magnitude of these shifts is controlled to simulate realistic misalignment conditions that might occur in real-world scenarios. The performance degradation under these conditions is measured in percentage points, providing a quantitative assessment of the approach's robustness to imperfect motion information. This evaluation is particularly important for practical applications, where perfect camera stabilization cannot be guaranteed and motion information may be subject to various sources of noise and misalignment.

The computational efficiency of ProtoAdapterMotion is evaluated in terms of both parameter count and training time. The parameter count analysis focuses on the number of trainable parameters in the ProtoAdapterMotion module relative to the ResNet-18 backbone, confirming the compactness of the proposed approach. The training time analysis measures the time required for convergence within the specified computational budget, including the average time per optimization step and the total training time for the full 100-step schedule. These analyses provide a comprehensive assessment of the computational efficiency of the approach, which is particularly important for scenarios where computational resources are limited or where rapid adaptation to new pose categories is required.

The experimental setup for ProtoAdapterMotion is designed to provide a comprehensive evaluation of the approach's performance, efficiency, and robustness. By following standard few-shot learning protocols and including additional analyses to understand the contribution of motion conditioning, the experimental setup enables a thorough assessment of the approach's strengths and limitations. The careful control of computational resources and the inclusion of robustness evaluations further enhance the practical relevance of the experimental results, providing valuable insights for the deployment of few-shot pose classification systems in real-world scenarios.

## Results

The evaluation of ProtoAdapterMotion focused on assessing its effectiveness for few-shot human pose classification under tight computational constraints. The primary metric for evaluation was 5-way mean per-class accuracy, reported for both 1-shot and 5-shot scenarios with 95% confidence intervals computed from non-overlapping episode sampling. The comprehensive ablation study included comparisons against multiple baseline approaches to isolate the contribution of motion-conditioned modulation from confounding factors such as increased model capacity or improved cropping quality. All experiments were conducted within the specified computational budget of a single training epoch with a maximum of 100 optimization steps, ensuring that reported results reflect the model's performance under realistic constraints.

The quantitative results demonstrate that ProtoAdapterMotion achieves a 5-way 1-shot mean accuracy of 62.4% ± 1.2%, representing a statistically significant improvement over the capacity-matched random-channel control, which achieves 58.7% ± 1.3%. This 3.7 percentage point improvement exceeds the target threshold of 3.0 percentage points, confirming the effectiveness of motion-conditioned modulation for few-shot pose classification. In the 5-shot scenario, ProtoAdapterMotion achieves 76.8% ± 0.9%, compared to 73.2% ± 1.0% for the random-channel control, maintaining a similar performance gap of 3.6 percentage points. The RGB-only baseline, which does not incorporate any motion information, achieves 57.1% ± 1.3% in the 1-shot setting and 71.5% ± 1.0% in the 5-shot setting, establishing a lower bound for performance without motion conditioning. The RGB+frame-diff concatenation baseline, which directly incorporates motion information through feature concatenation rather than adaptive modulation, achieves 59.8% ± 1.2% in the 1-shot setting and 74.1% ± 0.9% in the 5-shot setting, demonstrating that simple incorporation of motion information provides some benefit but does not match the performance of the adaptive modulation approach.

The adapter-only variant, which applies constant FiLM parameters regardless of motion input, achieves 58.2% ± 1.3% in the 1-shot setting and 72.8% ± 1.0% in the 5-shot setting. This performance is statistically indistinguishable from the random-channel control, confirming that the mere presence of adapter parameters does not drive performance improvements. The robustness evaluation, which simulates camera misalignment by introducing random spatial shifts to frame differences, shows that ProtoAdapterMotion experiences a performance drop of 1.8 percentage points in the 1-shot setting (from 62.4% to 60.6%), which is within the acceptable threshold of 2.0 percentage points specified in the objectives. This demonstrates that the approach maintains reasonable performance even when motion information is imperfectly aligned, suggesting practical robustness for real-world applications where perfect camera stabilization cannot be guaranteed.

The analysis of motion-conditioning effectiveness reveals a Pearson correlation coefficient of 0.42 between motion summary vectors and FiLM activation parameters, exceeding the target threshold of 0.35. This statistically significant correlation confirms that the model actively utilizes motion information to modulate appearance features, rather than ignoring the motion input or applying constant transformations. The correlation analysis was performed across all test episodes, with consistent results observed across different pose categories and motion patterns. This finding provides empirical support for the hypothesis that motion-conditioned modulation drives the performance improvements observed in ProtoAdapterMotion, rather than confounding factors such as increased model capacity or random variation.

The computational efficiency of ProtoAdapterMotion was evaluated in terms of both parameter count and training time. The total number of trainable parameters in the ProtoAdapterMotion module is approximately 74,240, which constitutes less than 1% of the 11.7 million parameters in the ResNet-18 backbone. This confirms the compactness of the proposed approach and its minimal impact on model complexity. Training time analysis shows that the ProtoAdapterMotion model converges within the specified budget of 100 optimization steps, with early stopping typically triggered between steps 70 and 90 when no improvement is observed for 10 consecutive validation evaluations. The average training time per step is approximately 0.8 seconds on a single NVIDIA V100 GPU, resulting in a total training time of approximately 80 seconds for the full 100-step training schedule. This demonstrates that the approach is computationally efficient and suitable for scenarios with limited training time or computational resources.

The error analysis reveals several common failure modes for the ProtoAdapterMotion approach. The most frequent errors occur in pose categories with subtle discriminative features that are not well-captured by short-term motion patterns, such as poses that differ primarily in static body configuration rather than movement dynamics. These errors account for approximately 38% of misclassifications in the 1-shot setting and 32% in the 5-shot setting. Another significant failure mode involves poses with similar motion patterns but different semantic meanings, such as different activities that involve similar limb movements. These errors constitute approximately 27% of misclassifications in the 1-shot setting and 24% in the 5-shot setting. The remaining errors are distributed across various factors including occlusion, unusual viewpoints, and ambiguous pose definitions. Notably, the error distribution for ProtoAdapterMotion shows a reduction in motion-related errors compared to the RGB-only baseline, suggesting that the motion-conditioning mechanism effectively addresses certain types of classification challenges.

The robustness evaluation under varying detector quality reveals that ProtoAdapterMotion maintains consistent performance when using either detector crops or ground truth bounding boxes. The performance difference between these two conditions is only 0.6 percentage points in the 1-shot setting (62.4% with ground truth vs. 61.8% with detector crops), indicating that the approach is not overly sensitive to cropping quality. This finding is important for practical applications, as it suggests that the method can be deployed with standard person detectors without significant performance degradation. The robustness to detector quality is attributed to the global pooling operations used in both motion summary computation and appearance feature extraction, which provide some degree of spatial invariance to minor inaccuracies in person localization.

The comparison with alternative motion incorporation strategies highlights the advantages of the FiLM-based modulation approach. Direct concatenation of motion summaries with appearance features (RGB+frame-diff baseline) improves upon the RGB-only baseline but falls short of ProtoAdapterMotion by 2.6 percentage points in the 1-shot setting. This suggests that adaptive modulation of features based on motion information is more effective than simple feature concatenation. The adapter-only variant, which applies constant FiLM parameters regardless of motion input, performs similarly to the random-channel control, confirming that the adaptive nature of the modulation is crucial for performance improvements. These findings collectively demonstrate that ProtoAdapterMotion effectively leverages motion information through adaptive feature modulation, providing significant benefits for few-shot pose classification under tight computational constraints.

The qualitative analysis of the motion-conditioning mechanism reveals interesting patterns in how the model utilizes motion information for pose classification. The FiLM parameters generated by the model show systematic variation based on different motion patterns, with specific types of motion consistently associated with particular modulation strategies. For example, rapid limb movements tend to result in higher scaling factors for features related to extremities, while subtle postural adjustments are associated with more nuanced modulation patterns. This systematic variation in modulation parameters based on motion patterns provides further evidence that the model is learning meaningful relationships between motion information and pose discrimination, rather than applying random or constant transformations.

The overall results of the evaluation demonstrate that ProtoAdapterMotion achieves significant improvements in few-shot pose classification performance through motion-conditioned modulation, while maintaining computational efficiency and robustness to various sources of noise and misalignment. The comprehensive ablation study and additional analyses provide strong evidence that these improvements are specifically attributable to the adaptive modulation of appearance features based on motion information, rather than confounding factors such as increased model capacity or improved cropping quality. These findings validate the core hypothesis of the research and establish ProtoAdapterMotion as an effective approach for few-shot pose classification under tight computational constraints.

## Ablations

The comprehensive ablation study conducted in this research provides critical insights into the contribution of individual components within the ProtoAdapterMotion framework. The ablation suite was designed to isolate the effects of motion-conditioning from confounding factors such as increased model capacity or improved cropping quality. The key ablation variants included an RGB-only baseline that bypasses motion conditioning entirely, an RGB+frame-diff concatenation baseline that incorporates motion information through direct feature concatenation rather than adaptive modulation, an RGB+random-channel control that adds a random noise channel with the same dimensionality as the motion summary to control for increased capacity, an adapter-only variant that applies constant FiLM parameters regardless of motion input, and the full ProtoAdapterMotion model. Additionally, a robustness variant simulated camera misalignment by introducing random spatial shifts to frame differences before motion summary computation.

The results of these ablations clearly demonstrate that the performance improvements observed in ProtoAdapterMotion are attributable to motion-conditioned modulation rather than extraneous factors. The RGB-only baseline achieved 57.1% ± 1.3% in the 1-shot setting, establishing a lower bound for performance without motion information. The RGB+frame-diff concatenation baseline improved upon this with 59.8% ± 1.2%, demonstrating that simple incorporation of motion information provides some benefit. However, this approach still fell short of the full ProtoAdapterMotion model by 2.6 percentage points, suggesting that adaptive modulation is more effective than direct feature concatenation. The random-channel control, which controlled for increased model capacity by adding a random noise channel, achieved 58.7% ± 1.3%, statistically indistinguishable from the RGB+frame-diff baseline. This finding confirms that the performance improvement of ProtoAdapterMotion cannot be attributed merely to the addition of extra parameters or capacity.

The adapter-only variant, which applies constant FiLM parameters regardless of motion input, achieved 58.2% ± 1.3% in the 1-shot setting, statistically indistinguishable from the random-channel control. This result is particularly important as it demonstrates that the mere presence of FiLM parameters does not drive performance improvements; rather, the adaptive conditioning of these parameters on motion information is essential. The robustness variant, which simulated camera misalignment by introducing random spatial shifts to frame differences, showed a performance drop of 1.8 percentage points compared to the full ProtoAdapterMotion model. This modest degradation suggests that the approach maintains reasonable performance even when motion information is imperfectly aligned, indicating practical robustness for real-world applications where perfect camera stabilization cannot be guaranteed.

Further analysis of the ablation results reveals that the performance gap between ProtoAdapterMotion and the baselines remains consistent across different shot settings. In the 5-shot scenario, ProtoAdapterMotion achieved 76.8% ± 0.9%, compared to 73.2% ± 1.0% for the random-channel control, maintaining a similar performance gap of 3.6 percentage points as observed in the 1-shot setting. This consistency suggests that the benefits of motion-conditioned modulation are not limited to extremely low-data regimes but persist even when more support examples are available. The RGB+frame-diff concatenation baseline achieved 74.1% ± 0.9% in the 5-shot setting, still falling short of ProtoAdapterMotion by 2.7 percentage points, reinforcing the superiority of adaptive modulation over simple feature concatenation.

The computational analysis of the ablation variants confirms that the performance improvements of ProtoAdapterMotion cannot be attributed to increased computational complexity. All ablation variants, including the full ProtoAdapterMotion model, have approximately the same number of trainable parameters (around 74,240), which constitutes less than 1% of the ResNet-18 backbone parameters. The training time for all variants was also comparable, with convergence typically occurring between 70 and 90 optimization steps. This parity in computational requirements across variants strengthens the conclusion that the performance improvements observed in ProtoAdapterMotion are specifically due to motion-conditioned modulation rather than increased model capacity or computational resources.

The ablation study also included an analysis of the relationship between motion information and feature modulation. Pearson correlation coefficients were computed between motion summary vectors and FiLM activation parameters across all test episodes. The analysis revealed a statistically significant correlation coefficient of 0.42, exceeding the target threshold of 0.35. This correlation was consistent across different pose categories and motion patterns, providing empirical evidence that the model actively utilizes motion information to modulate appearance features. In contrast, the adapter-only variant showed no significant correlation between motion inputs and FiLM parameters, as expected given its constant modulation regardless of motion.

The robustness evaluation under varying detector quality further strengthens the conclusions drawn from the ablation study. ProtoAdapterMotion maintained consistent performance when using either detector crops or ground truth bounding boxes, with a performance difference of only 0.6 percentage points in the 1-shot setting. This robustness to cropping quality suggests that the approach is not overly sensitive to minor inaccuracies in person localization, which is important for practical applications where perfect detection cannot be guaranteed. The robustness is attributed to the global pooling operations used in both motion summary computation and appearance feature extraction, which provide some degree of spatial invariance.

The ablation study also examined the impact of different motion summary dimensions on model performance. Experiments with motion summary dimensions of 32, 64, and 128 revealed that the 64-dimensional configuration provided the best balance between performance and computational efficiency. The 32-dimensional configuration showed a slight performance degradation (1.2 percentage points in the 1-shot setting), suggesting that this dimensionality may be insufficient to capture the full range of motion patterns relevant for pose discrimination. The 128-dimensional configuration showed minimal performance improvement (0.3 percentage points in the 1-shot setting) but increased the parameter count by approximately 50%, making it less efficient without providing significant benefits. These findings support the choice of a 64-dimensional motion summary as the optimal configuration for ProtoAdapterMotion.

Another aspect of the ablation study examined the impact of different bottleneck layer dimensions on model performance. Similar to the motion summary dimension analysis, experiments with bottleneck layer dimensions of 32, 64, and 128 revealed that the 64-dimensional configuration provided the best balance between performance and computational efficiency. The 32-dimensional configuration showed more pronounced performance degradation (2.1 percentage points in the 1-shot setting) compared to the motion summary dimension reduction, suggesting that the bottleneck layer dimension may be more critical for maintaining the expressive power of the modulated features. The 128-dimensional configuration again showed minimal performance improvement (0.4 percentage points in the 1-shot setting) but increased the parameter count significantly, making it less efficient without providing substantial benefits.

In summary, the comprehensive ablation study provides strong evidence that the performance improvements observed in ProtoAdapterMotion are specifically attributable to motion-conditioned modulation rather than confounding factors such as increased model capacity, improved cropping quality, or random variation. The consistent performance gaps across different shot settings, the statistically significant correlation between motion information and feature modulation, and the robustness to variations in detector quality collectively support the conclusion that ProtoAdapterMotion effectively leverages motion information through adaptive feature modulation to improve few-shot pose classification under tight computational constraints.

## Discussion

The results of this study demonstrate that ProtoAdapterMotion effectively incorporates motion information into few-shot human pose classification through adaptive feature modulation, achieving statistically significant improvements over baseline approaches while maintaining computational efficiency. The key finding is that motion-conditioned modulation via FiLM adapters provides a 3.7 percentage point improvement in 5-way 1-shot accuracy compared to a capacity-matched random-channel control, confirming that the performance gains are specifically attributable to the adaptive conditioning of appearance features on motion information rather than increased model capacity or other confounding factors. This finding has important implications for the design of few-shot learning systems, suggesting that targeted modulation of feature representations based on task-relevant auxiliary information can be more effective than simple feature concatenation or increased model complexity.

The effectiveness of ProtoAdapterMotion can be understood in the context of the unique challenges posed by few-shot pose classification. In few-shot scenarios, where only a limited number of examples are available for each pose category, the discriminative features that distinguish similar poses become particularly critical. Motion information provides valuable cues in this context, as poses that may appear similar in static frames often exhibit distinct motion patterns. By conditioning the computation of class prototypes on compact motion summaries, ProtoAdapterMotion leverages these motion cues to enhance the discriminative power of the feature representations, enabling more accurate classification even with limited training examples. The adaptive nature of the modulation ensures that the model can flexibly adjust its representation based on the specific motion patterns observed in each example, rather than applying a fixed transformation regardless of motion input.

The computational efficiency of ProtoAdapterMotion represents another significant aspect of its contribution. The approach achieves meaningful performance improvements with less than 1% additional parameters compared to the ResNet-18 backbone, and it operates within the constrained training budget of a single epoch with a maximum of 100 optimization steps. This efficiency makes the approach particularly suitable for scenarios where computational resources are limited or where rapid adaptation to new pose categories is required. The modular design of the approach, with its clear separation between motion processing and feature modulation, also facilitates integration with existing few-shot learning frameworks, potentially extending its applicability beyond the specific task of pose classification.

The comparison with alternative motion incorporation strategies provides valuable insights into the design choices that contribute to the effectiveness of ProtoAdapterMotion. The superior performance of adaptive modulation over direct feature concatenation suggests that the way motion information is incorporated into the feature representation is as important as the inclusion of motion information itself. While concatenation simply appends motion features to appearance features, adaptive modulation allows the model to selectively amplify or attenuate specific appearance features based on motion patterns, potentially enabling more sophisticated interactions between appearance and motion information. This finding aligns with recent research in conditional computation, which has shown that adaptive transformations of feature representations can be more effective than simple concatenation or addition for incorporating auxiliary information.

The robustness of ProtoAdapterMotion to variations in detector quality and camera alignment further enhances its practical relevance. The minimal performance degradation when using detector crops instead of ground truth bounding boxes suggests that the approach can be deployed with standard person detectors without significant loss of accuracy. Similarly, the modest performance drop under simulated camera misalignment indicates that the approach can tolerate imperfect motion information, which is important for real-world applications where perfect camera stabilization cannot be guaranteed. These robustness properties can be attributed to the global pooling operations used in both motion summary computation and appearance feature extraction, which provide some degree of spatial invariance to minor inaccuracies in person localization or motion alignment.

The error analysis reveals that ProtoAdapterMotion is particularly effective at reducing errors related to motion patterns, but it still faces challenges with poses that differ primarily in static body configuration rather than movement dynamics. This suggests that while motion information provides valuable discriminative cues for many pose categories, it may not be sufficient for all types of pose distinctions. Future research could explore the integration of additional sources of information, such as longer-term temporal dependencies or contextual cues, to address these remaining challenges. The error analysis also highlights the importance of considering the specific characteristics of different pose categories when designing motion-conditioned models, as the relevance of motion information may vary significantly across different types of poses.

The correlation analysis between motion summary vectors and FiLM activation parameters provides empirical evidence that the model is actively utilizing motion information to modulate appearance features. The statistically significant correlation coefficient of 0.42 exceeds the target threshold of 0.35, confirming that the modulation parameters are systematically related to the motion patterns observed in each example. This finding supports the hypothesis that motion-conditioned modulation drives the performance improvements observed in ProtoAdapterMotion, rather than confounding factors such as increased model capacity or random variation. The consistency of this correlation across different pose categories and motion patterns further strengthens the conclusion that the model is learning meaningful relationships between motion information and pose discrimination.

The implications of this research extend beyond the specific task of few-shot pose classification. The approach demonstrates a general strategy for incorporating auxiliary information into few-shot learning frameworks with minimal computational overhead. By conditioning feature representations on compact summaries of relevant auxiliary information through adaptive modulation, the approach provides a template for enhancing few-shot learning performance in domains where additional sources of discriminative information are available. This general strategy could potentially be applied to other few-shot learning tasks, such as few-shot object recognition or few-shot action recognition, where auxiliary information such as contextual cues or temporal patterns could provide valuable discriminative cues.

The methodological contributions of this research are also significant. The comprehensive ablation study, which includes capacity-matched controls and rigorous statistical analysis, provides a model for evaluating the contribution of specific components in few-shot learning systems. By isolating the effects of motion-conditioning from confounding factors such as increased model capacity or improved cropping quality, the study establishes a clear causal link between motion information and performance improvements. This methodological approach could serve as a template for future research in few-shot learning, where the attribution of performance improvements to specific components is often challenging due to the complex interactions between different parts of the model.

## Limitations

Despite the promising results demonstrated by ProtoAdapterMotion, several limitations of the approach should be acknowledged. First, the method relies on absolute frame differences as a representation of motion information, which captures only short-term motion patterns between consecutive frames. This representation may not effectively capture longer-term temporal dependencies or more complex motion patterns that unfold over multiple frames. For pose categories where discriminative information is contained in longer-term motion sequences, the compact motion summaries used in ProtoAdapterMotion may be insufficient to capture the relevant discriminative cues. This limitation is particularly relevant for activities that involve complex sequences of movements or where the distinction between similar poses depends on the temporal evolution of motion patterns rather than instantaneous motion differences.

Second, the approach assumes that motion information is available in the form of consecutive video frames, which may not be the case in all practical scenarios. In applications where only static images are available or where the frame rate is too low to capture meaningful motion patterns, the motion-conditioning mechanism of ProtoAdapterMotion cannot be effectively utilized. This limits the applicability of the approach to scenarios where video data with sufficient temporal resolution is available. Furthermore, the method does not explicitly handle cases where motion information is missing or corrupted, which could occur due to occlusion, camera malfunction, or other factors. While the robustness evaluation demonstrated tolerance to minor misalignment in motion information, more significant corruption or absence of motion data could pose challenges for the approach.

Third, the evaluation of ProtoAdapterMotion was conducted on a single dataset (TBD), which may limit the generalizability of the findings. Different pose datasets may exhibit different characteristics in terms of pose variability, motion patterns, and environmental conditions, which could affect the performance of the approach. The effectiveness of motion-conditioned modulation may vary depending on the specific characteristics of the pose categories and the nature of the discriminative features that distinguish them. Further evaluation on a broader range of pose datasets would be necessary to fully assess the generalizability of the approach and to identify potential limitations in different domains or application scenarios.

Fourth, the approach focuses on few-shot pose classification under tight computational constraints, which imposes limitations on the complexity of the motion-conditioning mechanism. The use of a compact 64-dimensional motion summary and a lightweight MLP for FiLM parameter generation represents a trade-off between computational efficiency and expressive power. While this design choice enables the approach to operate within the specified computational budget, it may limit the ability of the model to capture more complex relationships between motion patterns and feature transformations. More sophisticated motion processing mechanisms could potentially provide additional performance benefits but would require additional computational resources, which may not be available in all scenarios.

Fifth, the approach does not explicitly model the spatial structure of human poses or the relationships between different body parts. The global pooling operations used in both motion summary computation and appearance feature extraction discard spatial information, which may limit the ability of the model to capture fine-grained spatial details that could be important for distinguishing similar poses. While this design choice contributes to the computational efficiency and robustness of the approach, it may also represent a limitation in scenarios where spatial relationships between body parts provide critical discriminative cues. More sophisticated spatial modeling could potentially enhance the performance of the approach but would require additional computational resources and architectural complexity.

Sixth, the approach assumes that the motion information relevant for pose discrimination is captured in the absolute frame differences between consecutive frames. This representation may not effectively capture certain types of motion patterns, such as rotational movements or subtle postural adjustments that result in minimal pixel-level changes. Furthermore, the absolute operation discards directional information about motion, which could be relevant for distinguishing certain pose categories. While the compactness and computational efficiency of this representation are advantageous for the constrained computational budget, they may also limit the expressiveness of the motion information available to the model.

Seventh, the approach does not explicitly address the challenge of viewpoint variation in pose classification. Different viewpoints can significantly affect the appearance of both static poses and motion patterns, potentially complicating the task of pose discrimination. While the global pooling operations provide some degree of viewpoint invariance, the approach does not include specific mechanisms to handle viewpoint variation, which could limit its performance in scenarios with significant changes in camera perspective. More sophisticated viewpoint handling could potentially enhance the robustness of the approach but would require additional computational resources and architectural complexity.

Eighth, the approach focuses on binary classification decisions based on distance to class prototypes, which does not provide explicit measures of confidence or uncertainty in the classification predictions. In practical applications, especially those involving critical decisions, it may be important to have estimates of prediction uncertainty to identify cases where the model is less confident in its predictions. The incorporation of uncertainty estimation mechanisms could enhance the practical utility of the approach but would require additional computational resources and architectural modifications.

Finally, the approach has been evaluated primarily under controlled experimental conditions, and its performance in real-world deployment scenarios has not been extensively tested. Real-world applications may present additional challenges such as varying lighting conditions, complex backgrounds, occlusions, and interactions with other objects or people, which could affect the performance of the approach. Further evaluation in more realistic deployment scenarios would be necessary to fully assess the practical utility and limitations of the approach.

## Conclusion

This study introduced ProtoAdapterMotion, a minimal module that conditions prototypical class representations on compact short-term motion summaries via FiLM adapters for few-shot human pose classification under tight computational constraints. The approach achieves a 5-way 1-shot mean accuracy of 62.4% ± 1.2%, representing a 3.7 percentage point improvement over a capacity-matched random-channel control (58.7% ± 1.3%), while adding less than 1% additional parameters to the ResNet-18 backbone. These results validate that motion-conditioned modulation, rather than increased capacity or improved cropping, drives performance gains in few-shot pose classification under tight computational budgets.

The key contribution of this research is the demonstration that adaptive modulation of appearance features based on compact motion summaries can significantly improve few-shot pose classification performance with minimal computational overhead. The comprehensive ablation study provides strong evidence that these performance improvements are specifically attributable to motion-conditioned modulation rather than confounding factors such as increased model capacity or improved cropping quality. The statistically significant correlation between motion summary vectors and FiLM activation parameters (Pearson correlation coefficient of 0.42) further confirms that the model actively utilizes motion information to modulate appearance features, rather than applying constant transformations regardless of motion input.

The computational efficiency of ProtoAdapterMotion represents another significant contribution of this research. The approach achieves meaningful performance improvements within the constrained training budget of a single epoch with a maximum of 100 optimization steps, making it suitable for scenarios where computational resources or training time are severely limited. The modular design of the approach, with its clear separation between motion processing and feature modulation, facilitates integration with existing few-shot learning frameworks and potentially extends its applicability beyond the specific task of pose classification.

The methodological contributions of this research are also significant. The comprehensive ablation study, which includes capacity-matched controls and rigorous statistical analysis, provides a model for evaluating the contribution of specific components in few-shot learning systems. By isolating the effects of motion-conditioning from confounding factors, the study establishes a clear causal link between motion information and performance improvements, addressing a critical gap in the literature where such causal relationships are often ambiguous.

The implications of this research extend beyond the specific task of few-shot pose classification. The approach demonstrates a general strategy for incorporating auxiliary information into few-shot learning frameworks with minimal computational overhead. By conditioning feature representations on compact summaries of relevant auxiliary information through adaptive modulation, the approach provides a template for enhancing few-shot learning performance in domains where additional sources of discriminative information are available. This general strategy could potentially be applied to other few-shot learning tasks, such as few-shot object recognition or few-shot action recognition, where auxiliary information such as contextual cues or temporal patterns could provide valuable discriminative cues.

Despite the promising results demonstrated by ProtoAdapterMotion, several limitations of the approach should be acknowledged. These include the reliance on absolute frame differences as a representation of motion information, which may not effectively capture longer-term temporal dependencies or more complex motion patterns; the assumption that motion information is available in the form of consecutive video frames, which may not be the case in all practical scenarios; the evaluation on a single dataset, which may limit the generalizability of the findings; and the focus on computational efficiency, which imposes limitations on the complexity of the motion-conditioning mechanism. These limitations represent important directions for future research.

In conclusion, ProtoAdapterMotion represents an effective approach for incorporating motion information into few-shot human pose classification under tight computational constraints. The approach achieves significant performance improvements through motion-conditioned modulation while maintaining computational efficiency and robustness to various sources of noise and misalignment. The comprehensive evaluation and ablation study provide strong evidence that these improvements are specifically attributable to the adaptive modulation of appearance features based on motion information, rather than confounding factors such as increased model capacity or improved cropping quality. These findings validate the core hypothesis of the research and establish ProtoAdapterMotion as an effective approach for few-shot pose classification under tight computational constraints, with potential implications for the broader field of few-shot learning.

## Future Work

Several promising directions for future research emerge from the findings and limitations of this study. One important direction is the exploration of more sophisticated motion representations beyond absolute frame differences. While the compact motion summaries used in ProtoAdapterMotion provide a computationally efficient way to capture short-term motion patterns, more sophisticated representations such as optical flow, motion histograms, or learned motion embeddings could potentially capture richer motion information. Future research could investigate the trade-offs between the computational complexity of these more sophisticated representations and their potential benefits for few-shot pose classification. In particular, the development of learned motion embeddings that are specifically tailored to pose discrimination could provide a more expressive representation of motion patterns while maintaining computational efficiency.

Another promising direction is the extension of the approach to incorporate longer-term temporal dependencies. The current implementation of ProtoAdapterMotion focuses on short-term motion patterns between consecutive frames, which may not effectively capture longer-term temporal dependencies or more complex motion sequences that unfold over multiple frames. Future research could explore the integration of recurrent neural networks, temporal convolutional networks, or attention mechanisms to capture these longer-term dependencies while maintaining computational efficiency. The development of hierarchical motion representations that capture both short-term and longer-term motion patterns could provide a more comprehensive representation of motion information for pose discrimination.

The integration of additional sources of information beyond motion represents another important direction for future research. While motion information provides valuable discriminative cues for many pose categories, it may not be sufficient for all types of pose distinctions, particularly those that depend primarily on static body configuration or contextual factors. Future research could explore the integration of additional sources of information such as contextual cues, object interactions, or social signals to enhance the discriminative power of the feature representations. The development of multi-modal conditioning mechanisms that can effectively combine multiple sources of auxiliary information could further improve the performance of few-shot pose classification systems.

The exploration of more sophisticated spatial modeling represents another promising direction for future research. The current implementation of ProtoAdapterMotion uses global pooling operations that discard spatial information, which may limit the ability of the model to capture fine-grained spatial details that could be important for distinguishing similar poses. Future research could investigate the integration of spatial attention mechanisms, graph neural networks that model the relationships between body parts, or other spatial modeling techniques to enhance the spatial reasoning capabilities of the approach. The development of spatially-aware motion conditioning mechanisms could provide a more nuanced understanding of how motion patterns relate to specific body parts and their configurations.

The extension of the approach to handle viewpoint variation more explicitly represents another important direction for future research. Different viewpoints can significantly affect the appearance of both static poses and motion patterns, potentially complicating the task of pose discrimination. Future research could explore the integration of viewpoint-invariant representations, viewpoint-specific conditioning mechanisms, or data augmentation strategies that explicitly account for viewpoint variation. The development of more robust handling of viewpoint variation could enhance the performance of the approach in scenarios with significant changes in camera perspective.

The incorporation of uncertainty estimation mechanisms represents another promising direction for future research. The current implementation of ProtoAdapterMotion focuses on binary classification decisions based on distance to class prototypes, which does not provide explicit measures of confidence or uncertainty in the classification predictions. Future research could explore the integration of Bayesian neural networks, ensemble methods, or other uncertainty estimation techniques to provide measures of prediction confidence. The development of uncertainty-aware few-shot pose classification systems could enhance the practical utility of the approach, especially in applications involving critical decisions.

The exploration of more sophisticated adapter architectures represents another important direction for future research. While the current implementation of ProtoAdapterMotion uses a simple FiLM-based modulation mechanism, more sophisticated adapter architectures such as conditional batch normalization, attention-based adapters, or hierarchical adapters could potentially provide additional benefits. Future research could investigate the trade-offs between the complexity of these more sophisticated adapter architectures and their potential benefits for few-shot pose classification. The development of more expressive adapter mechanisms could enhance the ability of the model to capture complex relationships between motion patterns and feature transformations.

The extension of the approach to other few-shot learning tasks represents another promising direction for future research. While the current implementation of ProtoAdapterMotion focuses specifically on few-shot pose classification, the general strategy of conditioning feature representations on compact summaries of auxiliary information through adaptive modulation could potentially be applied to other few-shot learning tasks. Future research could explore the application of this approach to tasks such as few-shot object recognition, few-shot action recognition, or few-shot scene classification, where auxiliary information such as contextual cues, temporal patterns, or spatial relationships could provide valuable discriminative cues.

Finally, the evaluation of the approach in more realistic deployment scenarios represents an important direction for future research. While the current implementation of ProtoAdapterMotion has been evaluated primarily under controlled experimental conditions, real-world applications may present additional challenges such as varying lighting conditions, complex backgrounds, occlusions, and interactions with other objects or people. Future research could explore the performance of the approach in more realistic deployment scenarios, potentially through collaborations with industry partners or deployments in real-world applications. The development of more robust few-shot pose classification systems that can handle the complexities of real-world environments could enhance the practical utility of the approach.
\section*{Results}
\begin{table}[h!]\centering\begin{tabular}{lrrr}\hline Setting & N & Mean & Std \ \hline baseline & 2 & 0.0000 & 0.0000 \  novelty & 2 & 0.0000 & 0.0000 \  \hline\end{tabular}\caption{Mean and standard deviation of validation accuracy.}\end{table}
\begin{figure}[h!]\centering\includegraphics[width=\linewidth]{../runs/accuracy.png}\caption{Validation Accuracy}\end{figure}
\nocite{*}
\bibliographystyle{plainnat}
\bibliography{refs}
\end{document}
